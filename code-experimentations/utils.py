from math import *
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import graphviz as gv
import numbers
import random
import time
import sys

########## LABELED SET ##########

class LabeledSet:

    def __init__(self, input_dimension):
        '''
            initialize a labeled set with input dimension and ordinal attributes definition
        '''
        self.input_dimension = input_dimension
        self.nb_examples = 0

    def addExample(self,vector,label):
        '''
            vector : attribute values of example
            label : label of example
            add example to data set
        '''
        if (self.nb_examples == 0):
            self.x = np.array([vector])
            self.y = np.array([label])
        else:
            self.x = np.vstack((self.x, vector))
            self.y = np.vstack((self.y, label))

        self.nb_examples = self.nb_examples + 1

    def addExamples(self, vectors, labels):
        '''
            vectors : array of examples
            label : label of examples
            add examples to data set
        '''
        if (self.nb_examples == 0):
            self.x = vectors
            self.y = np.reshape(labels, (vectors.shape[0],1))
        else:
            self.x = np.vstack((self.x, vectors))
            self.y = np.vstack((self.y, labels))

        self.nb_examples += vectors.shape[0]

    def getInputDimension(self):
        return self.input_dimension

    def size(self):
        return self.nb_examples

    def getX(self, i):
        return self.x[i]

    def getY(self, i):
        return(self.y[i])

    def get_df(self):
        return self.df

########## F-LAYERS ##########

class F_layer:
    '''
        object-wise local monotonicity measure
    '''

    def value(self, w_i, labeled_set, dsa, dsl, esa, esl):
        raise NotImplementedError

    def equal_sets_attribute(self, labeled_set, a_j):
        '''
            a_j : index of attribute in labeled_set
            labeled_set : labeled set
            for each object w in labeled_set, return its equal set generated by a_j
        '''
        values = labeled_set.x.copy()
        ind = np.transpose(np.array([[i for i in range(labeled_set.size())]]))
        values = np.hstack((values, ind))
        n = labeled_set.size()

        equal_sets = np.zeros((n, n))
        for i in range(0, labeled_set.size()):
            v = labeled_set.getX(i)[a_j]
            equal = values[values[:,a_j] == v][:,labeled_set.getInputDimension()].astype(int)
            equal_sets[i][equal] = 1

        return equal_sets

    def equal_sets_label(self, labeled_set):
        '''
            labeled_set : labeled set
            for each object w in labeled_set, return its dominant set generated by its label
        '''
        values = labeled_set.y.copy()
        ind = np.transpose(np.array([[i for i in range(labeled_set.size())]]))
        values = np.hstack((values, ind))
        n = labeled_set.size()

        equal_sets = np.zeros((n, n))

        for i in range(0, labeled_set.size()):
            v = labeled_set.getY(i)
            equal = values[values[:,0] == v][:,1].astype(int)
            equal_sets[i][equal] = 1

        return equal_sets

    def dominant_sets_attribute(self, labeled_set, a_j):
        '''
            a_j : index of attribute in labeled_set
            labeled_set : labeled set
            for each object w in labeled_set, return its dominant set generated by a_j
        '''
        values = labeled_set.x.copy()
        ind = np.transpose(np.array([[i for i in range(labeled_set.size())]]))
        values = np.hstack((values, ind))
        n = labeled_set.size()

        dominant_sets = np.zeros((n, n))
        for i in range(0, labeled_set.size()):
            v = labeled_set.getX(i)[a_j]
            dominant = values[values[:,a_j] >= v][:,labeled_set.getInputDimension()].astype(int)
            dominant_sets[i][dominant] = 1

        return dominant_sets

    def dominant_sets_label(self, labeled_set):
        '''
            labeled_set : labeled set
            for each object w in labeled_set, return its dominant set generated by its label
        '''
        values = labeled_set.y.copy()
        ind = np.transpose(np.array([[i for i in range(labeled_set.size())]]))
        values = np.hstack((values, ind))
        n = labeled_set.size()

        dominant_sets = np.zeros((n, n))

        for i in range(0, labeled_set.size()):
            v = labeled_set.getY(i)
            dominant = values[values[:,0] >= v][:,1].astype(int)
            dominant_sets[i][dominant] = 1

        return dominant_sets

class Ds(F_layer):
    def value(self, w_i, labeled_set, dsa, dsl, esa, esl):
        '''
            labeled_set : labeled set
            w_i : index of object in labeled_set
            return ds value of w_i, a_j
        '''
        esa_i = np.argwhere(esa[w_i] == 1).flatten()
        esl_i = np.argwhere(esl[w_i] == 1).flatten()
        intersection = np.intersect1d(esa_i, esl_i)

        return intersection.size * 1.0 / esa_i.size

class Dsr(F_layer):

    def value(self, w_i, labeled_set, dsa, dsl, esa, esl):
        '''
            labeled_set : labeled set
            w_i : index of object in labeled_set
            return dsr value of w_i, a_j
        '''
        dsa_i = np.argwhere(dsa[w_i] == 1).flatten()
        dsl_i = np.argwhere(dsl[w_i] == 1).flatten()
        intersection = np.intersect1d(dsa_i, dsl_i)

        return intersection.size * 1.0 / dsa_i.size

class Minds(F_layer):

    def value(self, w_i, labeled_set, dsa, dsl, esa, esl):
        '''
            labeled_set : labeled set
            w_i : index of object in labeled_set
            return minds value of w_i, a_j
        '''
        n = labeled_set.size()
        equal_set = np.argwhere(esa[w_i] == 1).flatten()
        min_l = np.iinfo(np.int32).max

        for w_h in equal_set:
            esa_wh = np.argwhere(esa[w_h] == 1).flatten()
            esl_wh = np.argwhere(esl[w_i] == 1).flatten()

def generate_1Ddataset(k, n):
    '''
        k : number of classes
        n : number of examples to generate
        return a monotone dataset with k classes and n examples
    '''
    data = np.reshape(np.random.random_sample(n), (-1, 1))

    dataset = LabeledSet(1)

    for i in range(k):
        if i == 0:
            examples = data[(data <= 1.0/k)]
        else:
            examples = data[(data > (i*1.0)/k) & (data <= (i+1.0)/k)]

        examples = np.reshape(examples, (-1, 1))
        dataset.addExamples(examples, np.array([[i+1]] * examples.shape[0]))

    return dataset

def generate_2Ddataset(a_j, k, n, noise, amplitude, ranges, use_seed = False):
    '''
        a_j : monotone attribute
        k : number of labels
        n : number of examples to create
        noise :  % of non-monotone noise
        amplitude : amplitude of noise
        ranges : array of arrays indicating, for each attribute, its min and max values
        return 2D dataset containing k classes and n examples, with a_j being the monotone attribute
    '''
    labeled_set = LabeledSet(2)
    p = round(n/k)
    r = n # remaining examples to add

    current_min = ranges[a_j][0]
    total_range = ranges[a_j][1] - ranges[a_j][0]

    thresholds = []

    for q in range(k):
        current_max = current_min + (total_range / k)

        if (current_max > ranges[a_j][1]):
            current_max = ranges[a_j][1]

        if (current_max < ranges[a_j][1] and q == k-1):
            current_max = ranges[a_j][1]


        if (p < r) and (q==k-1):
            p = r

        if use_seed:
            seed = int(sys.argv[1])
            np.random.seed(seed)

        monotone_values = np.random.uniform(current_min, current_max, size=(p,1))

        if noise > 0:
            sample_size = np.random.binomial(len(monotone_values), noise)
            sample = np.random.randint(0, len(monotone_values), size=sample_size)

            for e in sample:
                if random.random() < 0.5:
                    val = current_min - random.uniform(0, total_range * amplitude)
                    if (val < ranges[a_j][0]):
                        val = ranges[a_j][0]
                    monotone_values[e] = val
                else:
                    val = current_max + random.uniform(0, total_range * amplitude)
                    if (val > ranges[a_j][1]):
                        val = ranges[a_j][1]
                    monotone_values[e] = val

        thresholds.append((current_min,current_max) )


        if (a_j == 0):
            random_values = np.random.uniform(ranges[1][0], ranges[1][1], size=(p, 1))
            values = np.hstack((monotone_values, random_values))
        else:
            random_values = np.random.uniform(ranges[0][0], ranges[0][1], size=(p,1))
            values = np.hstack((random_values, monotone_values))

        for i in range(p):
            labeled_set.addExample(values[i].tolist(), q+1)

        current_min = current_max
        r -= p
    return labeled_set, thresholds

def add_noise(labeled_set, noise):
    '''
        labeled_set : labeled set to add noise to (classes are equally distributed)
        noise : percentage of noise to add
        return labeled_set with specified amount of noise added
    '''
    p = int(np.count_nonzero(labeled_set.y == 1) * noise)  # number of examples to swap in each class
    labels = np.unique(labeled_set.y)
    k = max(labels)
    random_order = labels.copy() # define random order for permutation
    np.random.shuffle(random_order)
    noisy_set = LabeledSet(k)
    noisy_set.x = labeled_set.x.copy()
    noisy_set.y = labeled_set.y.copy()


    for i in range(len(random_order)):
        q1 = random_order[i]
        ind_q1 = np.where(noisy_set.y == q1)[0]
        sample = np.random.choice(ind_q1, p) # randomly pick p examples from class

        if i == k-1:
            q2 = random_order[0]
        else:
            q2 = random_order[i+1]

        noisy_set.y[sample] = np.array([[q2]] * p) # change labels

    return noisy_set



def normalize(x, min_v, max_v):
    return ((x - min_v)*1.0) / (max_v - min_v)

def generate_monotone_consistent_dataset(n, k, use_seed=True):
    '''
        n : number of examples to create
        k : class number
        generate 2D monotone consistent dataset with n examples and k classes
            with function f(x1, x2) = 1 + x1 + (1/2) (x2^2 - x1^2)
    '''

    if use_seed:
        seed = int(sys.argv[1])
        np.random.seed(seed)

    values = []
    x = []
    for i in range(n):
        x1 = np.random.uniform(0, 1)
        x2 = np.random.uniform(0, 1)
        x.append((x1, x2))
        v = 1 + x1 + (1/2) * (x2 * x2 - x1 * x1)
        values.append(v)

    max_v = max(values)
    min_v = min(values)
    results = []

    for i in range(n):
        results.append(normalize(values[i], min_v, max_v))

    x = [x for _, x in sorted(zip(results, x))]
    results = sorted(results)

    data = np.hstack((np.array(x), np.reshape(np.array([results]), (-1, 1))))

    monotone_set = LabeledSet(2)

    for i in range(k):
        if i == 0:
            examples = data[(data[:,2] <= 1.0/k)][:,:2]
            p = examples.shape[0]
        else:
            examples = data[(data[:,2] > (i*1.0)/k) & (data[:,2] <= (i+1.0)/k)][:,:2]
            p = examples.shape[0]
        monotone_set.addExamples(examples, np.array([[i+1]] * examples.shape[0]))

    return monotone_set

def NClash(x, y, labeled_set):
    '''
        labeled_set : labeled set
        x : example
        y : label of example
        return the number of examples that clash with x
    '''
    n = labeled_set.size()

    s = 0
    for i in range(n):
        z = labeled_set.getX(i)
        if (np.less_equal(z, x).all()) and (labeled_set.getY(i) > y):
            s += 1
        if (np.greater_equal(z, x).all()) and (labeled_set.getY(i) < y):
            s += 1
    return s

def clash(x, y, labeled_set):
    '''
        labeled_set : labeled set
        x : attribute values of example
        y : label of example
        return a set containing every elements in labeled_set (x,y) clashes with
    '''
    n = labeled_set.size()

    clashes = set()
    for i in range(n):
        z = labeled_set.getX(i)
        if (np.less_equal(z, x).all()) and (labeled_set.getY(i) > y):
            clashes.add(i)
        if (np.greater_equal(z, x).all()) and (labeled_set.getY(i) < y):
            clashes.add(i)
    return clashes

def update_clashes(labeled_set, clashes_dict, h, x, y):
    '''
        labeled_set : labeled set
        clashes_dict : dictionary {example index: set of examples that clash with it}
        h : index of element to be replaced
        x : attribute values of element replacing h
        y : label of element replacing h
        update dictionary according to specified parameters
    '''
    n = len(clashes_dict)

    new_clashes = clashes_dict.copy()
    # remove h from each entry if present and add new element to each entry it clashes with
    for i in range(n):
        x_i = labeled_set.getX(i)
        y_i = labeled_set.getY(i)

        if h not in new_clashes[i] and NMP(x_i, y_i, x, y): # add clash
            new_clashes[i].add(h)
        elif h in clashes_dict[i]:
            new_clashes[i].remove(h)

    return new_clashes

def NMI1_from_dict(clashes_dict):
    '''
        clashes_dict : dictionary of {x: set of examples that clash with x}
    '''
    c = 0
    n = len(clashes_dict)
    for k, s in clashes_dict.items():
        t = np.array(list(s))
        c += len(t[t > k]) # consider each pair only once
    return c / (n*(n-1))

def generate_noisy_monotone_dataset(n, lower_NMI, desired_NMI, m, k, f):
    '''
        n : number of examples
        desired_NMI : desired non-monotonicity index
        lower_NMI : lower bound of NMI
        m : number of attributes
        k : number of ordinal class values
        f : non-decreasing monotone function
        return a dataset with the above specifications
    '''
    class_num_values = []
    x = []

    # step A :
    #   for each example, assign random values to the attributes
    #   compute the output as f(attribute values)
    for i in range(n):
        vector = np.random.uniform(0,1,m)
        x.append(vector)
        c = f(vector)
        class_num_values.append(c)

    # step B :
    #   normalize output values and sort all the examples in increasing order of normalized output values
    max_v = max(class_num_values)
    min_v = min(class_num_values)
    outputs = []

    for i in range(n):
        outputs.append(normalize(class_num_values[i], min_v, max_v))

    x = [x for _, x in sorted(zip(outputs, x))]
    outputs = np.reshape(np.array([sorted(outputs)]), (-1, 1))

    # step C :
    #   assign ordinal values to the class such that the class values will be balanced
    data = np.hstack((np.array(x), np.reshape(np.array([outputs]), (-1, 1))))

    dataset = LabeledSet(m)

    for i in range(k):
        if i == 0:
            examples = data[(data[:,m] <= 1.0/k)][:,:m]
            p = examples.shape[0]
        else:
            examples = data[(data[:,m] > (i*1.0)/k) & (data[:,m] <= (i+1.0)/k)][:,:m]
            p = examples.shape[0]
        dataset.addExamples(examples, np.array([[i+1]] * p))

    current_NMI = NMI1(dataset)

    clashes = dict()

    for i in range(n):
        clashes[i] = clash(dataset.getX(i), dataset.getY(i), dataset)

    # step D
    while current_NMI < lower_NMI:
        r = random.sample(range(0, n), 2) # randomly select two examples

        # first example
        x = dataset.getX(r[0])
        y = dataset.getY(r[0])


        # generate a new example which clashes with the first example
        if y == 1:
            # randomly select attribute values for xp that are <= relative to x
            xp = np.random.uniform(0, x, m)
            # class value that is > to y
            xp_y = random.randint(2, k)
        elif y == k:
            # randomly select attribute values for xp that are >= relative to x
            xp = np.random.uniform(x, 1, m)
            # class value that is < to y
            xp_y = random.randint(1, k)
        else:
            if random.uniform(0, 1) <= 0.5:
                xp = np.random.uniform(0, x, m)
                xp_y = random.randint(y, k)
            else:
                xp = np.random.uniform(x, 1, m)
                xp_y = random.randint(1, y-1)

        # second example
        xs = dataset.getX(r[1])
        xs_y = dataset.getY(r[1])

        new_clashes = update_clashes(dataset, clashes, r[1], xp, xp_y)
        updated_NMI = NMI1_from_dict(new_clashes)

        if updated_NMI > current_NMI:
            # replace
            dataset.x[r[1]] = xp
            dataset.y[r[1]] = xp_y
            clashes = new_clashes.copy()
            current_NMI = updated_NMI

    return dataset

########## DISPLAY ##########

def plot2DSet(labeled_set, title):
    labels = list(set([item for sublist in labeled_set.y.tolist() for item in sublist]))
    mark_dict = {
        ".":"point",
        ",":"pixel",
        "o":"circle",
        "v":"triangle_down",
        "^":"triangle_up",
        "<":"triangle_left",
        ">":"triangle_right",
        "1":"tri_down",
        "2":"tri_up",
        "3":"tri_left",
        "4":"tri_right",
        "8":"octagon",
        "s":"square",
        "p":"pentagon",
        "*":"star",
        "h":"hexagon1",
        "H":"hexagon2",
        "+":"plus",
        "D":"diamond",
        "d":"thin_diamond",
        "|":"vline",
        "_":"hline"
    }
    S = []
    for label in labels:
        S.append(labeled_set.x[np.where(labeled_set.y == label),:][0])
    for i in range(len(labels)):
        plt.scatter(S[i][:,0],S[i][:,1],marker=list(mark_dict)[i])
    plt.xlabel("x")
    plt.ylabel("y")
    plt.title(title)

def display_discretization(labeled_set, threshold, a_j, title):
    '''
        labeled_set : labeled_set
        threshold : value of threshold
        a_j : index of d attribute
        title : plot title

        display 2D database along with threshold generated by discretization on attribute a_j
    '''
    plot2DSet(labeled_set, title)

    if (a_j == 0):
        max_v = ceil(max(labeled_set.x[:,1]))
        min_v = floor(min(labeled_set.x[:,1]))
        plt.plot([threshold, threshold], [min_v, max_v])
    else:
        max_v = ceil(max(labeled_set.x[:,0]))
        min_v = floor(min(labeled_set.x[:,0]))
        plt.plot([min_v, max_v], [threshold, threshold])

    plt.show()

def display_discretizations_comparison(labeled_set, threshold1, threshold2, real_thresholds, a_j, title, l1, l2):
    '''
        labeled_set : labeled_set
        threshold1 : threshold generated by discretization on a_j with first measure
        threshold2 : threshold generated by discretization on a_j with second measure
        real_thresholds : list of real thresholds
        title : plot title
        l1 : label of threshold1 (discrimination measure name)
        l2 : label of threshold2 (discrimination measure name)
        plot thresholds generated by two different discrimination measures on attribute a_j of labeled_set
    '''
    plot2DSet(labeled_set, title)

    if (a_j == 0):
        max_v = ceil(max(labeled_set.x[:,1]))
        min_v = floor(min(labeled_set.x[:,1]))
        plt.plot([threshold1, threshold1], [min_v, max_v], color='green', label=l1)
        plt.plot([threshold2, threshold2], [min_v, max_v], color='red', label=l2)
        for i in range(len(real_thresholds)):
            threshold = real_thresholds[i]
            if i == 0:
                plt.plot([threshold, threshold], [min_v, max_v], color='black', label="real threshold")
            else:
                plt.plot([threshold, threshold], [min_v, max_v], color='black')
    else:
        max_v = ceil(max(labeled_set.x[:,0]))
        min_v = floor(min(labeled_set.x[:,0]))
        plt.plot([min_v, max_v], [threshold1, threshold1], color='green', label=l1)
        plt.plot([min_v, max_v], [threshold2, threshold2], color='red', label=l2)
        for i in range(len(real_thresholds)):
            threshold = real_thresholds[i]
            if i == 0:
                plt.plot([min_v, max_v], [threshold, threshold], color='black', label="real threshold")
            else:
                plt.plot([min_v, max_v], [threshold, threshold], color='black')
    plt.legend()
    plt.show()

########## DATA SET SPLITTING ##########

def split_dataset(labeled_set, percentage):
    '''
        labeled_set : labeled set to split
        percentage : percentage of examples to put in the training set
        return labeled_set split into two subsets
    '''
    n = labeled_set.size()

    train_set = LabeledSet(labeled_set.getInputDimension())
    test_set = LabeledSet(labeled_set.getInputDimension())

    labels = np.unique(labeled_set.y)

    for k in labels:
        examples = labeled_set.x[np.where(labeled_set.y == k),:][0]
        np.random.shuffle(examples)
        p = int(examples.shape[0] * (percentage/100.0))
        train_set.addExamples(examples[:p,:], np.array([[k]] * p))
        test_set.addExamples(examples[p:,:], np.array([[k]] * (examples.shape[0] - p)))

    return train_set,test_set

def get_ten_folds(labeled_set):
    '''
        labeled_set : labeled set to split
        split labeled_set into ten folds
    '''
    sets = []
    labels = np.unique(labeled_set.y)
    k = labels.shape[0]
    n = labeled_set.size()
    starting_points = [0] * k
    ending_points = [-1] * k

    r = [0] * labels.shape[0] # remaining examples to add in each class

    for q in range(k):
        examples = labeled_set.x[np.where(labeled_set.y == labels[q]),:][0]
        r[q] = examples.shape[0]

    for i in range(10):
        dataset = LabeledSet(labeled_set.getInputDimension())

        for q in range(k):
            examples = labeled_set.x[np.where(labeled_set.y == labels[q]),:][0]
            p = int(round(examples.shape[0] / 10.0))
            if (i == 9) and (r[q] != p):
                ending_points[q] = examples.shape[0]
                r[q] = 0
            else:
                ending_points[q] = starting_points[q] + p
                r[q] -= p

            dataset.addExamples(examples[starting_points[q]:ending_points[q],:], np.array([[labels[q]]] * (ending_points[q] - starting_points[q])))


            starting_points[q] = ending_points[q]
        sets.append(dataset)
    return sets

def get_folds(labeled_set, nb):
    '''
        labeled_set : labeled set to split
        split labeled_set into ten folds
    '''
    sets = []
    labels = np.unique(labeled_set.y)
    k = labels.shape[0]
    n = labeled_set.size()
    starting_points = [0] * k
    ending_points = [-1] * k

    r = [0] * labels.shape[0] # remaining examples to add in each class

    for q in range(k):
        examples = labeled_set.x[np.where(labeled_set.y == labels[q]),:][0]
        r[q] = examples.shape[0]

    for i in range(nb):
        dataset = LabeledSet(labeled_set.getInputDimension())

        for q in range(k):
            examples = labeled_set.x[np.where(labeled_set.y == labels[q]),:][0]
            p = int(round(examples.shape[0] / nb))
            if (i == nb-1) and (r[q] != p):
                ending_points[q] = examples.shape[0]
                r[q] = 0
            else:
                ending_points[q] = starting_points[q] + p
                r[q] -= p

            dataset.addExamples(examples[starting_points[q]:ending_points[q],:], np.array([[labels[q]]] * (ending_points[q] - starting_points[q])))


            starting_points[q] = ending_points[q]
        sets.append(dataset)
    return sets

