from math import *
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import graphviz as gv
import numbers
import random
import time
import sys

########## LABELED SET ##########

class LabeledSet:

    def __init__(self, input_dimension):
        '''
            initialize a labeled set with input dimension and ordinal attributes definition
        '''
        self.input_dimension = input_dimension
        self.nb_examples = 0

    def addExample(self,vector,label):
        '''
            vector : attribute values of example
            label : label of example
            add example to data set
        '''
        if (self.nb_examples == 0):
            self.x = np.array([vector])
            self.y = np.array([label])
        else:
            self.x = np.vstack((self.x, vector))
            self.y = np.vstack((self.y, label))

        self.nb_examples = self.nb_examples + 1

    def addExamples(self, vectors, labels):
        '''
            vectors : array of examples
            label : label of examples
            add examples to data set
        '''
        if (self.nb_examples == 0):
            self.x = vectors
            self.y = np.reshape(labels, (vectors.shape[0],1))
        else:
            self.x = np.vstack((self.x, vectors))
            self.y = np.vstack((self.y, labels))

        self.nb_examples += vectors.shape[0]

    def getInputDimension(self):
        return self.input_dimension

    def size(self):
        return self.nb_examples

    def getX(self, i):
        return self.x[i]

    def getY(self, i):
        return(self.y[i])

    def get_df(self):
        return self.df

########## F-LAYERS ##########

class F_layer:
    '''
        object-wise local monotonicity measure
    '''

    def value(self, w_i, labeled_set, dsa, dsl, esa, esl):
        raise NotImplementedError

    def equal_sets_attribute(self, labeled_set, a_j):
        '''
            a_j : index of attribute in labeled_set
            labeled_set : labeled set
            for each object w in labeled_set, return its equal set generated by a_j
        '''
        values = labeled_set.x.copy()
        ind = np.transpose(np.array([[i for i in range(labeled_set.size())]]))
        values = np.hstack((values, ind))
        n = labeled_set.size()

        equal_sets = np.zeros((n, n))
        for i in range(0, labeled_set.size()):
            v = labeled_set.getX(i)[a_j]
            equal = values[values[:,a_j] == v][:,labeled_set.getInputDimension()].astype(int)
            equal_sets[i][equal] = 1

        return equal_sets

    def equal_sets_label(self, labeled_set):
        '''
            labeled_set : labeled set
            for each object w in labeled_set, return its dominant set generated by its label
        '''
        values = labeled_set.y.copy()
        ind = np.transpose(np.array([[i for i in range(labeled_set.size())]]))
        values = np.hstack((values, ind))
        n = labeled_set.size()

        equal_sets = np.zeros((n, n))

        for i in range(0, labeled_set.size()):
            v = labeled_set.getY(i)
            equal = values[values[:,0] == v][:,1].astype(int)
            equal_sets[i][equal] = 1

        return equal_sets

    def dominant_sets_attribute(self, labeled_set, a_j):
        '''
            a_j : index of attribute in labeled_set
            labeled_set : labeled set
            for each object w in labeled_set, return its dominant set generated by a_j
        '''
        values = labeled_set.x.copy()
        ind = np.transpose(np.array([[i for i in range(labeled_set.size())]]))
        values = np.hstack((values, ind))
        n = labeled_set.size()

        dominant_sets = np.zeros((n, n))
        for i in range(0, labeled_set.size()):
            v = labeled_set.getX(i)[a_j]
            dominant = values[values[:,a_j] >= v][:,labeled_set.getInputDimension()].astype(int)
            dominant_sets[i][dominant] = 1

        return dominant_sets

    def dominant_sets_label(self, labeled_set):
        '''
            labeled_set : labeled set
            for each object w in labeled_set, return its dominant set generated by its label
        '''
        values = labeled_set.y.copy()
        ind = np.transpose(np.array([[i for i in range(labeled_set.size())]]))
        values = np.hstack((values, ind))
        n = labeled_set.size()

        dominant_sets = np.zeros((n, n))

        for i in range(0, labeled_set.size()):
            v = labeled_set.getY(i)
            dominant = values[values[:,0] >= v][:,1].astype(int)
            dominant_sets[i][dominant] = 1

        return dominant_sets

class Ds(F_layer):
    def value(self, w_i, labeled_set, dsa, dsl, esa, esl):
        '''
            labeled_set : labeled set
            w_i : index of object in labeled_set
            return ds value of w_i, a_j
        '''
        esa_i = np.argwhere(esa[w_i] == 1).flatten()
        esl_i = np.argwhere(esl[w_i] == 1).flatten()
        intersection = np.intersect1d(esa_i, esl_i)

        return intersection.size * 1.0 / esa_i.size

class Dsr(F_layer):

    def value(self, w_i, labeled_set, dsa, dsl, esa, esl):
        '''
            labeled_set : labeled set
            w_i : index of object in labeled_set
            return dsr value of w_i, a_j
        '''
        dsa_i = np.argwhere(dsa[w_i] == 1).flatten()
        dsl_i = np.argwhere(dsl[w_i] == 1).flatten()
        intersection = np.intersect1d(dsa_i, dsl_i)

        return intersection.size * 1.0 / dsa_i.size

class Minds(F_layer):

    def value(self, w_i, labeled_set, dsa, dsl, esa, esl):
        '''
            labeled_set : labeled set
            w_i : index of object in labeled_set
            return minds value of w_i, a_j
        '''
        n = labeled_set.size()
        equal_set = np.argwhere(esa[w_i] == 1).flatten()
        min_l = np.iinfo(np.int32).max

        for w_h in equal_set:
            esa_wh = np.argwhere(esa[w_h] == 1).flatten()
            esl_wh = np.argwhere(esl[w_i] == 1).flatten()
            l = (np.intersect1d(esa_wh, esl_wh).size)
            if min_l > l:
                min_l = l

        return min_l * 1.0 / equal_set.size

class Mindsr(F_layer):
    def value(self, w_i, labeled_set, dsa, dsl, esa, esl):
        '''
            labeled_set : labeled set
            w_i : index of object in labeled_set
            dsa : dictionnary of dominant sets generated by a_j
            dsl : dictionnary of dominant sets generated by a_j
            esa : dictionnary of equal sets generated by a_j
            return mindsr value of w_i, a_j
        '''
        n = labeled_set.size()
        dominant_set = np.argwhere(dsa[w_i] == 1).flatten()
        equal_set = np.argwhere(esa[w_i] == 1).flatten()
        min_l = np.iinfo(np.int32).max

        for w_h in equal_set:
            dsa_wh = np.argwhere(dsa[w_h] == 1).flatten()
            dsl_wh = np.argwhere(dsl[w_h] == 1).flatten()
            l = np.intersect1d(dsa_wh, dsl_wh).size
            if min_l > l:
                min_l = l

        return min_l * 1.0 / dominant_set.size

class Maxds(F_layer):

    def value(self, w_i, labeled_set, dsa, dsl, esa, esl):
        '''
            labeled_set : labeled set
            w_i : index of object in labeled_set
            dsa : dictionnary of dominant sets generated by a_j
            dsl : dictionnary of dominant sets generated by a_j
            esa : dictionnary of equal sets generated by a_j
            return maxds value of w_i, a_j
        '''
        n = labeled_set.size()
        intersections_lengths = []

        equal_set = np.argwhere(esa[w_i] == 1).flatten()
        max_l = np.iinfo(np.int32).min

        for w_h in equal_set:
            esa_wh = np.argwhere(esa[w_h] == 1).flatten()
            esl_wh = np.argwhere(esl[w_h] == 1).flatten()
            l = np.intersect1d(esa_wh, esl_wh).size
            if max_l < l:
                max_l = l

        return max_l * 1.0 / equal_set.size

class Maxdsr(F_layer):
    def value(self, w_i, labeled_set, dsa, dsl, esa, esl):
        '''
            labeled_set : labeled set
            w_i : index of object in labeled_set
            dsa : dictionnary of dominant sets generated by a_j
            dsl : dictionnary of dominant sets generated by a_j
            esa : dictionnary of equal sets generated by a_j
            return maxdsr value of w_i, a_j
        '''
        n = labeled_set.size()
        dominant_set = np.argwhere(dsa[w_i] == 1).flatten()
        equal_set = np.argwhere(esa[w_i] == 1).flatten()
        max_l = np.iinfo(np.int32).min

        for w_h in equal_set:
            dsa_wh = np.argwhere(dsa[w_h] == 1).flatten()
            dsl_wh = np.argwhere(dsl[w_h] == 1).flatten()
            l = np.intersect1d(dsa_wh, dsl_wh).size
            if max_l < l:
                max_l = l

        return max_l * 1.0 / dominant_set.size

class Avgds(F_layer):
    def value(self, w_i, labeled_set, dsa, dsl, esa, esl):
        '''
            labeled_set : labeled set
            w_i : index of object in labeled_set
            return avgds value of w_i, a_j
        '''
        n = labeled_set.size()
        s = 0

        equal_set = np.argwhere(esa[w_i] == 1).flatten()

        for w_h in equal_set:
            esa_wh = np.argwhere(esa[w_h] == 1).flatten()
            esl_wh = np.argwhere(esl[w_h] == 1).flatten()
            s += np.intersect1d(esa_wh, esl_wh).size

        return ((1.0/equal_set.size)* s) / (1.0 * equal_set.size)

class Avgdsr(F_layer):
    def value(self, w_i, labeled_set, dsa, dsl, esa, esl):
        '''
            labeled_set : labeled set
            w_i : index of object in labeled_set
            dsa : dictionnary of dominant sets generated by a_j
            dsl : dictionnary of dominant sets generated by a_j
            esa : dictionnary of equal sets generated by a_j
            return avgdsr value of w_i, a_j
        '''
        n = labeled_set.size()
        s = 0

        dominant_set = np.argwhere(dsa[w_i] == 1).flatten()
        equal_set = np.argwhere(esa[w_i] == 1).flatten()

        for w_h in equal_set:
            dsa_wh = np.argwhere(dsa[w_h] == 1).flatten()
            dsl_wh = np.argwhere(dsl[w_h] == 1).flatten()
            s += np.intersect1d(dsa_wh, dsl_wh).size

        return ((1.0/equal_set.size) * s) / (dominant_set.size * 1.0)

########## G-LAYERS ##########

class G_layer:
    '''
        object-wise local non-monotonicity measure
    '''

    def value(self, f_value):
        raise NotImplementedError

class Log(G_layer):
    def value(self, f_value):
        '''
            f_value : value computed by f_layer
            return -log_2(f_value)
        '''
        return -log(f_value, 2)

class One_minus(G_layer):
    def value(self, f_value):
        '''
            f_value : value computed by f_layer
            return 1 - f_value
        '''
        return 1 - f_value

class One_minus_square(G_layer):
    def value(self, f_value):
        return 1 - (f_value * f_value)

class Frac(G_layer):
    def value(self, f_value):
        '''
            f_value : value computed by f_layer
            return -log(f_value) / f_value
        '''
        return -log(f_value, 2) / (1.0 * f_value)

########## H-LAYERS ##########

class H_layer:
    '''
        aggregated local non-monotonicity measure
    '''

    def value(self, g_values, labeled_set):
        raise NotImplementedError

class Sum(H_layer):
    def value(self, g_values, labeled_set):
        '''
            return (1/labeled_set.size()) * sum(g_values)
        '''
        return (1.0/labeled_set.size()) * np.sum(g_values)

class Max(H_layer):
    def value(self, g_values, labeled_set):
        return np.max(g_values)

class Square_root(H_layer):
    def value(self, g_values, labeled_set):
        return sqrt((1.0/labeled_set.size()) * np.sum(np.power(g_values, 2)))

########## GENERIC DISCRIMINATION MEASURE ##########

class Gdm:
    '''
        Generic rank discrimination measure
    '''
    def __init__(self, h, g, f):
        '''
            h : object-wise local monotonicity measure
            g : object-wise local non-monotonicity measure
            f : aggregated local non-monotonicity measure
            labeled_set : labeled set
        '''
        self.h = h
        self.g = g
        self.f = f

    def value(self, labeled_set, a_j, dsa, dsl, esa, esl):
        g_f = []


        for i in range(0, labeled_set.size()):
            g_f.append(self.g.value(self.f.value(i, labeled_set, dsa, dsl, esa, esl)))

        return self.h.value(g_f, labeled_set)

########## DECISION TREE CONSTRUCTION ##########

def discretize(H, labeled_set, a_j):
    '''
        H : discrimation measure
        labeled_set : labeled set
        a_j : attribute to discretize
        return threshold which minimizes the entropy function H for given feature a_j,
            along with minimum entropy value
    '''

    n = labeled_set.size()

    unique_values = np.unique(labeled_set.x[:,a_j])
    values = np.hstack((labeled_set.x[:,[a_j]], labeled_set.y))
    values = np.hstack((values, np.reshape(np.arange(n), (n,1))))
    # first column : value of attribute, second column : label, third column: index of element in labeled_set
    sorted_values = np.sort(values.view([('',values.dtype)]*values.shape[1]),0).view(values.dtype) # sort values according to first column

    df = pd.DataFrame(sorted_values)
    grouped = df.groupby(df[0], axis=0)

    # binary set : for each object w_i taken in ascending order of a_j value,
    # a_j(w_h) = 0 if a_j(w_h) <= a_j(w_i), 1 otherwise
    binary_set = LabeledSet(labeled_set.getInputDimension())
    binary_set.nb_examples = labeled_set.size()
    binary_set.x = np.ones(labeled_set.size())
    binary_set.y = labeled_set.y

    thresholds = []
    H_values = []

    dsa = np.ones((n, n))
    dsl = H.f.dominant_sets_label(binary_set)
    esa = np.ones((n, n))
    esl = H.f.equal_sets_label(binary_set)

    visited_ind = []
    total_ind = np.arange(0, n)



    for k in range(len(unique_values)-1):
        current = unique_values[k] # current attribute value
        current_group = grouped.groups[current].values # indices of elements in df sharing this value
        current_labels = np.unique(sorted_values[current_group, 1]) # labels sharing this value

        lookahead = unique_values[k+1]
        lookahead_group = grouped.groups[lookahead]
        lookahead_labels = np.unique(sorted_values[lookahead_group,1])

        visited_ind.extend(sorted_values[current_group, 2].astype(int).tolist())

        binary_set.x[sorted_values[current_group, 2].astype(int)] = 0

        if np.array_equal(current_labels,lookahead_labels):
            if current_labels.size == 1:
                continue

        a = np.zeros((n,))

        a[visited_ind] = 1
        dsa[visited_ind] = np.ones((n,))
        esa[visited_ind] = a

        notvisited_ind = list(set(total_ind) - set(visited_ind))

        a = np.zeros((n,))
        a[notvisited_ind] = 1

        dsa[notvisited_ind] = a
        esa[notvisited_ind] = a

        thresholds.append((current + lookahead) / 2.0)
        H_values.append(H.value(binary_set, a_j, dsa, dsl, esa, esl))


    min_entropy = min(H_values)
    min_threshold = thresholds[np.argmin(H_values)]

    return (min_threshold, min_entropy)

def discretize_steps(H, labeled_set, a_j):
    '''
        H : discrimation measure
        labeled_set : labeled set
        a_j : attribute to discretize
        return discretization steps
    '''

    n = labeled_set.size()

    unique_values = np.unique(labeled_set.x[:,a_j])
    values = np.hstack((labeled_set.x[:,[a_j]], labeled_set.y))
    values = np.hstack((values, np.reshape(np.arange(n), (n,1))))
    # first column : value of attribute, second column : label, third column: index of element in labeled_set
    sorted_values = np.sort(values.view([('',values.dtype)]*values.shape[1]),0).view(values.dtype) # sort values according to first column

    df = pd.DataFrame(sorted_values)
    grouped = df.groupby(df[0], axis=0)

    # binary set : for each object w_i taken in ascending order of a_j value,
    # a_j(w_h) = 0 if a_j(w_h) <= a_j(w_i), 1 otherwise
    binary_set = LabeledSet(labeled_set.getInputDimension())
    binary_set.nb_examples = labeled_set.size()
    binary_set.x = np.ones(labeled_set.size())
    binary_set.y = labeled_set.y

    thresholds = []
    H_values = []

    dsa = np.ones((n, n))
    dsl = H.f.dominant_sets_label(binary_set)
    esa = np.ones((n, n))
    esl = H.f.equal_sets_label(binary_set)

    visited_ind = []
    total_ind = np.arange(0, n)



    for k in range(len(unique_values)-1):
        current = unique_values[k] # current attribute value
        current_group = grouped.groups[current].values # indices of elements in df sharing this value
        current_labels = np.unique(sorted_values[current_group, 1]) # labels sharing this value

        lookahead = unique_values[k+1]
        lookahead_group = grouped.groups[lookahead]
        lookahead_labels = np.unique(sorted_values[lookahead_group,1])

        visited_ind.extend(sorted_values[current_group, 2].astype(int).tolist())

        binary_set.x[sorted_values[current_group, 2].astype(int)] = 0

        if np.array_equal(current_labels,lookahead_labels):
            if current_labels.size == 1:
                continue

        a = np.zeros((n,))

        a[visited_ind] = 1
        dsa[visited_ind] = np.ones((n,))
        esa[visited_ind] = a

        notvisited_ind = list(set(total_ind) - set(visited_ind))

        a = np.zeros((n,))
        a[notvisited_ind] = 1

        dsa[notvisited_ind] = a
        esa[notvisited_ind] = a

        thresholds.append((current + lookahead) / 2.0)
        H_values.append(H.value(binary_set, a_j, dsa, dsl, esa, esl))


    return (thresholds, H_values)



def majority_class(labeled_set, labels):
    '''
        labeled_set : labeled set
        label : list of labels
        return majority class in labeled_set
    '''
    classes_size = []

    for label in labels:
        classes_size.append(len(labeled_set.x[np.where(labeled_set.y == label),:][0]))

    classes_size = np.array(classes_size)
    return labels[np.random.choice(np.flatnonzero(classes_size == classes_size.max()))]
    #return labels[np.argmax(np.array(classes_size))]

def assign_label(labeled_set, take_majority, position):
    labels = np.sort(np.unique(labeled_set.y)).astype(int).tolist()
    k = len(labels)

    if take_majority:
        return majority_class(labeled_set, labels)
    else:
        if k == 1:
            return labels[0]
        elif k % 2 == 0:
            if position == 'left':
                return labels[int(k/2)-1]
            else:
                return labels[int(k/2)]
        else:
            return labels[int(k/2)]

def constant_lambda(labeled_set):
    '''
        labeled_set : labeled set
        return true if all objects in labeled_set share the same label, false otherwise
    '''
    labels = labeled_set.y
    return np.all(labels == labels[0,:], axis=0)[0]

def shannon(P):
    '''
        P : class distribution
        compute Shannon entropy
    '''
    Hs = 0
    k = len(P)
    for p_i in P:
        tmp = 0
        if p_i != 0:
            tmp = p_i * log(p_i, k)
        Hs += tmp

    return -Hs

def entropy(labeledSet, labels, name):
    P = []

    # get class distribution
    for label in labels:
        P.append(len(labeledSet.x[np.where(labeledSet.y == label),0:labeledSet.getInputDimension()][0]) / (1.0 * labeledSet.size()))

    # shannon entropy
    return shannon(P)

def divide(Lset, att, threshold):
    '''
        Lset : labeled_set
        att : index of attribute to divide
        threshold : threshold value
        divide Lset into two sub-sets : one with values for att <= threshold, one with values > threshold
    '''
    m = Lset.getInputDimension()
    E1 = LabeledSet(m)
    E2 = LabeledSet(m)

    data = np.hstack((Lset.x, Lset.y))

    E1.addExamples(data[data[:,att] <= threshold][:,:m], data[data[:,att] <= threshold][:,m])
    E2.addExamples(data[data[:,att] > threshold][:,:m], data[data[:,att] > threshold][:,m])

    return E1, E2

class BinaryTree:
    '''
        Binary tree
        deal with numeric attributes (ordinal attributes have to be pre-treated and must be orderable)
        deal with multi-class classification (classes must be orderable)
    '''
    def __init__(self):
        self.attribute = None

        # leaf
        self.label = None
        self.labeled_set = None

        # internal node
        self.threshold = None
        self.inf = None
        self.sup = None

    def isLeaf(self):
        """
            return True if tree is a leaf
        """
        return self.attribute == None

    def add_children(self, inf, sup, att, threshold):
        """
            inf, sup : trees
            att : index of attribute
            threshold : threshold value
            add children to node
        """
        self.attribute = att
        self.threshold = threshold
        self.inf = inf
        self.sup = sup

    def addLeaf(self,label, labeled_set):
        """
            add leaf corresponding to label
        """
        self.label = label
        self.labeled_set = labeled_set

    def classify(self,example):
        """
            example : numpy array in labeled set
            classify example
        """
        if self.isLeaf():
            return self.label
        else:
            if example[self.attribute] <= self.threshold:
                return self.inf.classify(example)
            return self.sup.classify(example)

    def to_graph(self, g, prefix='A'):
        """
            build a representation of the tree
        """
        if self.isLeaf():
            g.node(prefix,str(self.label),shape='box')
        else:
            g.node(prefix, str(self.attribute))

            g.node(prefix, str(self.attribute))
            self.inf.to_graph(g,prefix+"l")
            self.sup.to_graph(g,prefix+"r")
            g.edge(prefix,prefix+"l", '<='+ str(round(self.threshold,2)))
            g.edge(prefix,prefix+"r", '>'+ str(round(self.threshold, 2)))
        return g

    def get_depth(self):
        '''
            return tree depth
        '''
        if self.isLeaf():
            return 1
        else:
            return 1 + max(self.inf.get_depth(), self.sup.get_depth())

    def get_nb_leaves(self):
        '''
            return number of leaves
        '''
        if self.isLeaf():
            return 1
        else:
            return self.inf.get_nb_leaves() + self.sup.get_nb_leaves()

    def get_nb_nodes(self):
        '''
            return total number of nodes
        '''
        if self.isLeaf():
            return 0
        return self.inf.get_nb_nodes() + self.sup.get_nb_nodes() + 1

    def get_total_pairs(self):
        '''
            return number of pairs of leaves
        '''
        if (self.inf.isLeaf()) and (self.sup.isLeaf()):
            return 1
        elif self.inf.isLeaf():
            return self.sup.get_total_pairs()
        elif self.sup.isLeaf():
            return self.inf.get_total_pairs()
        else:
            return self.inf.get_total_pairs() + self.sup.get_total_pairs()

    def is_rule_monotone(self):
        '''
            return true if tree is rule monotone, false otherwise
        '''
        if (self.inf.isLeaf()) and (self.sup.isLeaf()):
            if self.inf.label > self.sup.label:
                return False
            return True
        elif self.inf.isLeaf():
            return self.sup.is_rule_monotone()
        elif self.sup.isLeaf():
            return self.inf.is_rule_monotone()
        else:
            return self.inf.is_rule_monotone() and self.sup.is_rule_monotone()

    def get_ratio_non_monotone_pairs(self):
        '''
            if both children of current node are leaves,
            compute tuple of :
            - ratio between number of non-monotone pairs
            (e.g (w_i, w_h) in (left node, right node) where lambda(w_i) > lambda(w_h))
            and total number of pairs of examles, multiplied by total number of examples
            - total number of examples in both leaves
        '''
        if (self.inf.isLeaf()) and (self.sup.isLeaf()):

            inf_set = self.inf.labeled_set
            sup_set = self.sup.labeled_set

            n_inf = inf_set.size()
            n_sup = sup_set.size()

            c = 0
            for i in range(n_inf):
                for j in range(n_sup):
                    if inf_set.getY(i) > sup_set.getY(j):
                        c += 1
            #return [((n_inf + n_sup) * (c * 1.0) / (n_inf * n_sup), n_inf +n_sup, c, (n_inf + n_sup)/(n_inf * n_sup))]
            #return [((n_inf + n_sup) * (c * 1.0) / (n_inf * n_sup), n_inf + n_sup)]
            return [(c / (n_inf * n_sup), n_inf + n_sup, (n_inf + n_sup)/(n_inf * n_sup))]

        elif self.inf.isLeaf():
            return self.sup.get_ratio_non_monotone_pairs()

        elif self.sup.isLeaf():
            return self.inf.get_ratio_non_monotone_pairs()

        else:
            t_inf = self.inf.get_ratio_non_monotone_pairs()
            t_sup = self.sup.get_ratio_non_monotone_pairs()


            t_inf.extend(t_sup)
            return t_inf


    def evaluate_NMI1(self):
        if (self.inf.isLeaf()) and (self.sup.isLeaf()):
            inf_set = self.inf.labeled_set
            sup_set = self.sup.labeled_set

            dataset = LabeledSet(inf_set.getInputDimension())
            dataset.addExamples(inf_set.x, inf_set.y)
            dataset.addExamples(sup_set.x, sup_set.y)

            n_inf = inf_set.size()
            n_sup = sup_set.size()

            nmi = NMI1(dataset)
            return [((n_inf + n_sup) * (nmi * 1.0) / (n_inf * n_sup), n_inf + n_sup)]

        elif self.inf.isLeaf():
            return self.sup.evaluate_NMI1()

        elif self.sup.isLeaf():
            return self.inf.evaluate_NMI1()

        else:
            t_inf = self.inf.evaluate_NMI1()
            t_sup = self.sup.evaluate_NMI1()


            t_inf.extend(t_sup)
            return t_inf



    def evaluate_monotonicity(self):
        '''
            if both children of current node are leaves,
            compute rank shannon discrimination measure value of node
        '''
        if (self.inf.isLeaf()) and (self.sup.isLeaf()):
            g = Log()
            h = Sum()
            f = Dsr()
            rsdm = Gdm(h, g, f) # rank Shannon discrimination measure

            a_j = self.attribute
            inf_set = self.inf.labeled_set
            sup_set = self.sup.labeled_set

            n_inf = inf_set.size()
            n_sup = sup_set.size()

            total_set = LabeledSet(inf_set.getInputDimension())
            total_set.addExamples(inf_set.x, inf_set.y)
            total_set.addExamples(sup_set.x, sup_set.y)

            dsa = rsdm.f.dominant_sets_attribute(total_set, a_j)
            dsl = rsdm.f.dominant_sets_label(total_set)
            esa = rsdm.f.equal_sets_attribute(total_set, a_j)
            esl = rsdm.f.equal_sets_label(total_set)

            h = rsdm.value(total_set, a_j, dsa, dsl, esa, esl)

            return [(h * (n_inf + n_sup), n_inf + n_sup)]

        elif self.inf.isLeaf():
            return self.sup.evaluate_monotonicity()
        elif self.sup.isLeaf():
            return self.inf.evaluate_monotonicity()
        else:
            h_inf = self.inf.evaluate_monotonicity()
            h_sup = self.sup.evaluate_monotonicity()

            h_inf.extend(h_sup)
            return h_inf


    def get_leaves(self):
        if self.isLeaf():
            values = self.labeled_set.x
            a = values.min(axis=0)
            b = values.max(axis=0)
            return [(self.label, a, b)]
        else:
            t = self.inf.get_leaves()
            t.extend(self.sup.get_leaves())
            return t

def build_DT(labeled_set, H, H_stop, measureThreshold, maxDepth, minSize, labels, current_depth, position, take_majority=False):
    '''
        labeled_set : labeled set
        H : rank discrimination measure used for discretization
        H_stop : discrimination measure (shannon, gini ...) used for stopping condition
        measure_threshold : lower bound for H_stop
        max_depth : maximum length of a path from the root to a leaf node
        minSize : sets the minimum size of the current object set labeled_set
        position : left or right (child of parent node)
        build decision tree recursively
    '''
    h = entropy(labeled_set, labels, "shannon")
    if (h <= measureThreshold) or (labeled_set.size() <= minSize) or (constant_lambda(labeled_set)) or (current_depth > maxDepth):
        leaf = BinaryTree()
        #leaf.addLeaf(majority_class(labeled_set, labels), labeled_set)
        leaf.addLeaf(assign_label(labeled_set, take_majority, position), labeled_set)
        return leaf

    m = labeled_set.getInputDimension()
    min_threshold = None
    min_attribute = None

    h_values = []
    thresholds = []

    for a_j in range(m):
        # all objects share the same value for attribute a_j
        if np.all(labeled_set.x == labeled_set.x[0,:], axis = 0)[a_j]:
            thresholds.append(None)
            h_values.append(np.iinfo(np.int32).max)
            continue

        threshold, h = discretize(H, labeled_set, a_j)
        thresholds.append(threshold)
        h_values.append(h)

    if all(thr is None for thr in thresholds):
        leaf = BinaryTree()
        #leaf.addLeaf(majority_class(labeled_set, labels), labeled_set)
        leaf.addLeaf(assign_label(labeled_set, take_majority, position), labeled_set)
        return leaf


    min_threshold = thresholds[np.argmin(h_values)]
    min_attribute = np.argmin(h_values)

    inf_set, sup_set = divide(labeled_set, min_attribute, min_threshold)
    bt = BinaryTree()

    if inf_set.size() == 0:
        #bt.addLeaf(majority_class(sup_set, labels), sup_set)
        bt.addLeaf(assign_label(labeled_set, take_majority, position), sup_set)
        return bt
    if sup_set.size() == 0:
        #bt.addLeaf(majority_class(inf_set, labels), inf_set)
        bt.addLeaf(assign_label(labeled_set, take_majority, position), inf_set)
        return bt

    inf_bt = build_DT(inf_set, H, H_stop, measureThreshold, maxDepth, minSize, labels, current_depth+1, "left", take_majority)
    sup_bt = build_DT(sup_set, H, H_stop, measureThreshold, maxDepth, minSize, labels, current_depth+1, "right", take_majority)
    bt.add_children(inf_bt, sup_bt, min_attribute, min_threshold)
    return bt

########## CLASSIFIERS ##########

class Classifier:
    def __init__(self,input_dimension):
        raise NotImplementedError("Please Implement this method")

    def predict(self,x):
        '''
            x : example
            compute prediction on x => return score
        '''
        raise NotImplementedError("Please Implement this method")

    def train(self,labeled_set):
        '''
            labeled_set : labeled set
            train model on labeled_set
        '''
        raise NotImplementedError("Please Implement this method")

    def accuracy(self,labeled_set):
        '''
            labeled_set : labeled_set
            return accuracy score on whole dataset
        '''
        nb_ok=0
        for i in range(labeled_set.size()):
            score = self.predict(labeled_set.getX(i))
            if (score == labeled_set.getY(i)):
                nb_ok = nb_ok+1
        acc = nb_ok/(labeled_set.size() * 1.0)
        return acc

class RDMT(Classifier):
    '''
        Rank discrimination measure tree
    '''
    def __init__(self, H, H_stop, measureThreshold, maxDepth, percMinSize, labels):
        '''
            H : discrimination measure to minimize for splitting
            H_stop : discrimination measure (shannon, gini ...) used for stopping condition
            measureThreshold : lower bound for the discrimination measure H
            maxDepth : maximum length of a path from the root to a leaf node
            percMinSize : minimum size of the current object set
            labels : list of classes
        '''
        self.H = H
        self.H_stop = H_stop
        self.measureThreshold = measureThreshold
        self.maxDepth = maxDepth
        self.percMinSize = percMinSize
        self.labels = labels
        self.root = None

    def predict(self,x):
        '''
            classify x using RDMT
            return prediction
        '''
        label = self.root.classify(x)
        return label

    def train(self,labeled_set):
        '''
            set : training set
            builds RDMT using set
        '''
        self.labeled_set = labeled_set
        self.root = build_DT(labeled_set,self.H, self.H_stop, self.measureThreshold, self.maxDepth, self.percMinSize, self.labels, 0, None)

    def plot(self):
        '''
            display tree
        '''
        gtree = gv.Digraph(format='png')
        return self.root.to_graph(gtree)

    def leaves(self):
        return self.root.get_leaves()

    def get_depth(self):
        return self.root.get_depth()

    def get_nb_leaves(self):
        return self.root.get_nb_leaves()

    def is_rule_monotone(self):
        return self.root.is_rule_monotone()

    def get_ratio_non_monotone_pairs(self):
        '''
            return average ratio between number of pairwise non-monotone label comparisons and number of pairs
        '''
        # first column : ratio computed for each pair
        # second column : number of examples involved in each pair
        #n = self.labeled_set.size()
        #t = np.array(self.root.get_ratio_non_monotone_pairs())
        #n_eval = t[:,1].sum()
        #r = t[:,0].sum() / t[:, 1].sum()
        #return (t[:,0].sum() / t[:, 1].sum()) / t.shape[0]
        #return ((n - n_eval) + r * n_eval) / n, t[:,2], t[:,3]
        #return ((n - n_eval) + r * n_eval) / n

        t = np.array(self.root.get_ratio_non_monotone_pairs())
        r_i = t[:,0]
        n_i = t[:,1]
        return ((n_i * r_i).sum()) / (n_i.sum()), t[:,2]

    def evaluate_NMI1(self):
        t = np.array(self.root.evaluate_NMI1())
        n_eval = t[:,1].sum()
        r = t[:,0].sum() / n_eval
        return r

    def evaluate_monotonicity(self):
        '''
            return average monotonicity evaluation (computed by rank shannon discrimination measure) of RDMT
        '''
        evaluations = np.array(self.root.evaluate_monotonicity())
        n_evaluation = evaluations[:,1].sum()
        return np.mean(evaluations[:,0]) / n_evaluation

    def get_total_pairs(self):
        '''
            return total number of pairs of leaves used for ratio computing
        '''
        #t = np.array(self.root.get_ratio_non_monotone_pairs())
        #return t.shape[0]
        return self.root.get_total_pairs()

    def get_nb_nodes(self):
        return self.root.get_nb_nodes()

    def pairs_ratio(self):
        '''
            return ratio between number of pair of leaves and total number of nodes
        '''
        return self.get_total_pairs() / self.get_nb_nodes()

    def get_total_examples_ratio(self):
        '''
            return total number of examples used for ratio computing
        '''
        t = np.array(self.root.get_ratio_non_monotone_pairs())
        return t[:,1].sum()

    def MAE(self, labeled_set):
        '''
            labeled_set : labeled set for evaluating the performance of the algorithm
            return mean absolute error
        '''
        s = 0
        n = labeled_set.size()
        for i in range(n):
            x = labeled_set.getX(i)
            y = labeled_set.getY(i)
            s += fabs(self.predict(x) - y)
        return (1.0/n) * s


########## QUANTIFICATION OF NON-MONOTONICITY ##########

def NMP(x1, y1, x2, y2):
    '''
        x1 : attribute values of first example
        y1 : label of first example
        x2 : attribute values of second example
        y2 : label of second example
        return 1 if x1 <= x2 and y1 > y2 or x1 >= x2 and y1 < y2, 0 otherwise
    '''

    if np.array_equal(x1, x2):
        if y1 != y2:
            return 1
        return 0
    else:
        if np.all(np.greater_equal(x2, x1)):
            if y2 < y1: # x1 <= x2 and y1 > y2
                return 1
            else:
                return 0
        if np.all(np.greater_equal(x1, x2)):
            if y1 < y2: # x2 <= x1 and y1 < y2
                return 1
            else:
                return 0

        return 0 # incomparable pair

def NMI1(labeled_set):
    '''
        labeled_set : labeled set
        return index of non-monotonicity NMI1
    '''
    n = labeled_set.size()

    s = 0
    for i in range(n):
        for j in range(n):
            x1 = labeled_set.getX(i)
            x2 = labeled_set.getX(j)
            y1 = labeled_set.getY(i)
            y2 = labeled_set.getY(j)
            s += NMP(x1, y1, x2, y2)

    return s / (n*n - n)

def NMP_leaves(a1, b1, y1, a2, b2, y2):
    '''
        a1, b1 : min and max example in first leaf
        a2, b2 : min and max example in second leaf
        y1 : label of first leaf
        y2 : label of second leaf
        return 1 if specified pair of leaves is non-monotone, 0 otherwise
    '''
    if (np.sum(a1 - b2) < 0 and y1 > y2) or (np.sum(b1 - a2) > 0 and y1 < y2):
        return 1
    return 0

def non_monotonicity_matrix(T):
    '''
        T : decision tree
        return non-monotonicity matrix
    '''
    leaves = T.leaves()
    q = len(leaves)
    M = np.zeros((q, q))
    for i in range(q):
        a1 = leaves[i][1] # min
        b1 = leaves[i][2] # max
        y1 = leaves[i][0] # label
        for j in range(i+1, q):
            a2 = leaves[j][1]
            b2 = leaves[j][2]
            y2 = leaves[j][0]
            if NMP_leaves(a1, b1, y1, a2, b2, y2):
                print(leaves[i])
                print(leaves[j])
                M[i][j] = 1
    return M

def I_tree(T):
    '''
        T : decision tree
        compute the non-monotonicity of T
    '''
    leaves = T.leaves()
    q = len(leaves)
    M = non_monotonicity_matrix(T)
    return np.count_nonzero(M) / (q*q - q)

########## DATA SET GENERATION ##########

def generate_1Ddataset(k, n):
    '''
        k : number of classes
        n : number of examples to generate
        return a monotone dataset with k classes and n examples
    '''
    data = np.reshape(np.random.random_sample(n), (-1, 1))

    dataset = LabeledSet(1)

    for i in range(k):
        if i == 0:
            examples = data[(data <= 1.0/k)]
        else:
            examples = data[(data > (i*1.0)/k) & (data <= (i+1.0)/k)]

        examples = np.reshape(examples, (-1, 1))
        dataset.addExamples(examples, np.array([[i+1]] * examples.shape[0]))

    return dataset

def generate_2Ddataset(a_j, k, n, noise, amplitude, ranges, use_seed = False):
    '''
        a_j : monotone attribute
        k : number of labels
        n : number of examples to create
        noise :  % of non-monotone noise
        amplitude : amplitude of noise
        ranges : array of arrays indicating, for each attribute, its min and max values
        return 2D dataset containing k classes and n examples, with a_j being the monotone attribute
    '''
    labeled_set = LabeledSet(2)
    p = round(n/k)
    r = n # remaining examples to add

    current_min = ranges[a_j][0]
    total_range = ranges[a_j][1] - ranges[a_j][0]

    thresholds = []

    for q in range(k):
        current_max = current_min + (total_range / k)

        if (current_max > ranges[a_j][1]):
            current_max = ranges[a_j][1]

        if (current_max < ranges[a_j][1] and q == k-1):
            current_max = ranges[a_j][1]


        if (p < r) and (q==k-1):
            p = r


        monotone_values = np.random.uniform(current_min, current_max, size=(p,1))

        if noise > 0:
            sample_size = np.random.binomial(len(monotone_values), noise)
            sample = np.random.randint(0, len(monotone_values), size=sample_size)

            for e in sample:
                if random.random() < 0.5:
                    val = current_min - random.uniform(0, total_range * amplitude)
                    if (val < ranges[a_j][0]):
                        val = ranges[a_j][0]
                    monotone_values[e] = val
                else:
                    val = current_max + random.uniform(0, total_range * amplitude)
                    if (val > ranges[a_j][1]):
                        val = ranges[a_j][1]
                    monotone_values[e] = val

        thresholds.append((current_min,current_max) )

        if use_seed:
            seed = int(sys.argv[1])
            np.random.seed(seed)

        if (a_j == 0):
            random_values = np.random.uniform(ranges[1][0], ranges[1][1], size=(p, 1))
            values = np.hstack((monotone_values, random_values))
        else:
            random_values = np.random.uniform(ranges[0][0], ranges[0][1], size=(p,1))
            values = np.hstack((random_values, monotone_values))

        for i in range(p):
            labeled_set.addExample(values[i].tolist(), q+1)

        current_min = current_max
        r -= p
    return labeled_set, thresholds

def add_noise(labeled_set, noise):
    '''
        labeled_set : labeled set to add noise to (classes are equally distributed)
        noise : percentage of noise to add
        return labeled_set with specified amount of noise added
    '''
    p = int(np.count_nonzero(labeled_set.y == 1) * noise)  # number of examples to swap in each class
    labels = np.unique(labeled_set.y)
    k = max(labels)
    random_order = labels.copy() # define random order for permutation
    np.random.shuffle(random_order)
    noisy_set = LabeledSet(k)
    noisy_set.x = labeled_set.x.copy()
    noisy_set.y = labeled_set.y.copy()


    for i in range(len(random_order)):
        q1 = random_order[i]
        ind_q1 = np.where(noisy_set.y == q1)[0]
        sample = np.random.choice(ind_q1, p) # randomly pick p examples from class

        if i == k-1:
            q2 = random_order[0]
        else:
            q2 = random_order[i+1]

        noisy_set.y[sample] = np.array([[q2]] * p) # change labels

    return noisy_set



def normalize(x, min_v, max_v):
    return ((x - min_v)*1.0) / (max_v - min_v)

def generate_monotone_consistent_dataset(n, k):
    '''
        n : number of examples to create
        k : class number
        generate 2D monotone consistent dataset with n examples and k classes
            with function f(x1, x2) = 1 + x1 + (1/2) (x2^2 - x1^2)
    '''
    values = []
    x = []
    for i in range(n):
        x1 = random.uniform(0, 1)
        x2 = random.uniform(0, 1)
        x.append((x1, x2))
        v = 1 + x1 + (1/2) * (x2 * x2 - x1 * x1)
        values.append(v)

    max_v = max(values)
    min_v = min(values)
    results = []

    for i in range(n):
        results.append(normalize(values[i], min_v, max_v))

    x = [x for _, x in sorted(zip(results, x))]
    results = sorted(results)

    data = np.hstack((np.array(x), np.reshape(np.array([results]), (-1, 1))))

    monotone_set = LabeledSet(2)

    for i in range(k):
        if i == 0:
            examples = data[(data[:,2] <= 1.0/k)][:,:2]
            p = examples.shape[0]
        else:
            examples = data[(data[:,2] > (i*1.0)/k) & (data[:,2] <= (i+1.0)/k)][:,:2]
            p = examples.shape[0]
        monotone_set.addExamples(examples, i+1)

    return monotone_set

def NClash(x, y, labeled_set):
    '''
        labeled_set : labeled set
        x : example
        y : label of example
        return the number of examples that clash with x
    '''
    n = labeled_set.size()

    s = 0
    for i in range(n):
        z = labeled_set.getX(i)
        if (np.less_equal(z, x).all()) and (labeled_set.getY(i) > y):
            s += 1
        if (np.greater_equal(z, x).all()) and (labeled_set.getY(i) < y):
            s += 1
    return s

def clash(x, y, labeled_set):
    '''
        labeled_set : labeled set
        x : attribute values of example
        y : label of example
        return a set containing every elements in labeled_set (x,y) clashes with
    '''
    n = labeled_set.size()

    clashes = set()
    for i in range(n):
        z = labeled_set.getX(i)
        if (np.less_equal(z, x).all()) and (labeled_set.getY(i) > y):
            clashes.add(i)
        if (np.greater_equal(z, x).all()) and (labeled_set.getY(i) < y):
            clashes.add(i)
    return clashes

def update_clashes(labeled_set, clashes_dict, h, x, y):
    '''
        labeled_set : labeled set
        clashes_dict : dictionary {example index: set of examples that clash with it}
        h : index of element to be replaced
        x : attribute values of element replacing h
        y : label of element replacing h
        update dictionary according to specified parameters
    '''
    n = len(clashes_dict)

    new_clashes = clashes_dict.copy()
    # remove h from each entry if present and add new element to each entry it clashes with
    for i in range(n):
        x_i = labeled_set.getX(i)
        y_i = labeled_set.getY(i)

        if h not in new_clashes[i] and NMP(x_i, y_i, x, y): # add clash
            new_clashes[i].add(h)
        elif h in clashes_dict[i]:
            new_clashes[i].remove(h)

    return new_clashes

def NMI1_from_dict(clashes_dict):
    '''
        clashes_dict : dictionary of {x: set of examples that clash with x}
    '''
    c = 0
    n = len(clashes_dict)
    for k, s in clashes_dict.items():
        t = np.array(list(s))
        c += len(t[t > k]) # consider each pair only once
    return c / (n*(n-1))

def generate_noisy_monotone_dataset(n, lower_NMI, desired_NMI, m, k, f):
    '''
        n : number of examples
        desired_NMI : desired non-monotonicity index
        lower_NMI : lower bound of NMI
        m : number of attributes
        k : number of ordinal class values
        f : non-decreasing monotone function
        return a dataset with the above specifications
    '''
    class_num_values = []
    x = []

    # step A :
    #   for each example, assign random values to the attributes
    #   compute the output as f(attribute values)
    for i in range(n):
        vector = np.random.uniform(0,1,m)
        x.append(vector)
        c = f(vector)
        class_num_values.append(c)

    # step B :
    #   normalize output values and sort all the examples in increasing order of normalized output values
    max_v = max(class_num_values)
    min_v = min(class_num_values)
    outputs = []

    for i in range(n):
        outputs.append(normalize(class_num_values[i], min_v, max_v))

    x = [x for _, x in sorted(zip(outputs, x))]
    outputs = np.reshape(np.array([sorted(outputs)]), (-1, 1))

    # step C :
    #   assign ordinal values to the class such that the class values will be balanced
    data = np.hstack((np.array(x), np.reshape(np.array([outputs]), (-1, 1))))

    dataset = LabeledSet(m)

    for i in range(k):
        if i == 0:
            examples = data[(data[:,m] <= 1.0/k)][:,:m]
            p = examples.shape[0]
        else:
            examples = data[(data[:,m] > (i*1.0)/k) & (data[:,m] <= (i+1.0)/k)][:,:m]
            p = examples.shape[0]
        dataset.addExamples(examples, np.array([[i+1]] * p))

    current_NMI = NMI1(dataset)

    clashes = dict()

    for i in range(n):
        clashes[i] = clash(dataset.getX(i), dataset.getY(i), dataset)

    # step D
    while current_NMI < lower_NMI:
        r = random.sample(range(0, n), 2) # randomly select two examples

        # first example
        x = dataset.getX(r[0])
        y = dataset.getY(r[0])


        # generate a new example which clashes with the first example
        if y == 1:
            # randomly select attribute values for xp that are <= relative to x
            xp = np.random.uniform(0, x, m)
            # class value that is > to y
            xp_y = random.randint(2, k)
        elif y == k:
            # randomly select attribute values for xp that are >= relative to x
            xp = np.random.uniform(x, 1, m)
            # class value that is < to y
            xp_y = random.randint(1, k)
        else:
            if random.uniform(0, 1) <= 0.5:
                xp = np.random.uniform(0, x, m)
                xp_y = random.randint(y, k)
            else:
                xp = np.random.uniform(x, 1, m)
                xp_y = random.randint(1, y-1)

        # second example
        xs = dataset.getX(r[1])
        xs_y = dataset.getY(r[1])

        new_clashes = update_clashes(dataset, clashes, r[1], xp, xp_y)
        updated_NMI = NMI1_from_dict(new_clashes)

        if updated_NMI > current_NMI:
            # replace
            dataset.x[r[1]] = xp
            dataset.y[r[1]] = xp_y
            clashes = new_clashes.copy()
            current_NMI = updated_NMI

    return dataset

########## DISPLAY ##########

def plot2DSet(labeled_set, title):
    labels = list(set([item for sublist in labeled_set.y.tolist() for item in sublist]))
    mark_dict = {
        ".":"point",
        ",":"pixel",
        "o":"circle",
        "v":"triangle_down",
        "^":"triangle_up",
        "<":"triangle_left",
        ">":"triangle_right",
        "1":"tri_down",
        "2":"tri_up",
        "3":"tri_left",
        "4":"tri_right",
        "8":"octagon",
        "s":"square",
        "p":"pentagon",
        "*":"star",
        "h":"hexagon1",
        "H":"hexagon2",
        "+":"plus",
        "D":"diamond",
        "d":"thin_diamond",
        "|":"vline",
        "_":"hline"
    }
    S = []
    for label in labels:
        S.append(labeled_set.x[np.where(labeled_set.y == label),:][0])
    for i in range(len(labels)):
        plt.scatter(S[i][:,0],S[i][:,1],marker=list(mark_dict)[i])
    plt.xlabel("x")
    plt.ylabel("y")
    plt.title(title)

def display_discretization(labeled_set, threshold, a_j, title):
    '''
        labeled_set : labeled_set
        threshold : value of threshold
        a_j : index of d attribute
        title : plot title

        display 2D database along with threshold generated by discretization on attribute a_j
    '''
    plot2DSet(labeled_set, title)

    if (a_j == 0):
        max_v = ceil(max(labeled_set.x[:,1]))
        min_v = floor(min(labeled_set.x[:,1]))
        plt.plot([threshold, threshold], [min_v, max_v])
    else:
        max_v = ceil(max(labeled_set.x[:,0]))
        min_v = floor(min(labeled_set.x[:,0]))
        plt.plot([min_v, max_v], [threshold, threshold])

    plt.show()

def display_discretizations_comparison(labeled_set, threshold1, threshold2, real_thresholds, a_j, title, l1, l2):
    '''
        labeled_set : labeled_set
        threshold1 : threshold generated by discretization on a_j with first measure
        threshold2 : threshold generated by discretization on a_j with second measure
        real_thresholds : list of real thresholds
        title : plot title
        l1 : label of threshold1 (discrimination measure name)
        l2 : label of threshold2 (discrimination measure name)
        plot thresholds generated by two different discrimination measures on attribute a_j of labeled_set
    '''
    plot2DSet(labeled_set, title)

    if (a_j == 0):
        max_v = ceil(max(labeled_set.x[:,1]))
        min_v = floor(min(labeled_set.x[:,1]))
        plt.plot([threshold1, threshold1], [min_v, max_v], color='green', label=l1)
        plt.plot([threshold2, threshold2], [min_v, max_v], color='red', label=l2)
        for i in range(len(real_thresholds)):
            threshold = real_thresholds[i]
            if i == 0:
                plt.plot([threshold, threshold], [min_v, max_v], color='black', label="real threshold")
            else:
                plt.plot([threshold, threshold], [min_v, max_v], color='black')
    else:
        max_v = ceil(max(labeled_set.x[:,0]))
        min_v = floor(min(labeled_set.x[:,0]))
        plt.plot([min_v, max_v], [threshold1, threshold1], color='green', label=l1)
        plt.plot([min_v, max_v], [threshold2, threshold2], color='red', label=l2)
        for i in range(len(real_thresholds)):
            threshold = real_thresholds[i]
            if i == 0:
                plt.plot([min_v, max_v], [threshold, threshold], color='black', label="real threshold")
            else:
                plt.plot([min_v, max_v], [threshold, threshold], color='black')
    plt.legend()
    plt.show()

########## DATA SET SPLITTING ##########

def split_dataset(labeled_set, percentage):
    '''
        labeled_set : labeled set to split
        percentage : percentage of examples to put in the training set
        return labeled_set split into two subsets
    '''
    n = labeled_set.size()

    train_set = LabeledSet(labeled_set.getInputDimension())
    test_set = LabeledSet(labeled_set.getInputDimension())

    labels = np.unique(labeled_set.y)

    for k in labels:
        examples = labeled_set.x[np.where(labeled_set.y == k),:][0]
        np.random.shuffle(examples)
        p = int(examples.shape[0] * (percentage/100.0))
        train_set.addExamples(examples[:p,:], np.array([[k]] * p))
        test_set.addExamples(examples[p:,:], np.array([[k]] * (examples.shape[0] - p)))

    return train_set,test_set

def get_ten_folds(labeled_set):
    '''
        labeled_set : labeled set to split
        split labeled_set into ten folds
    '''
    sets = []
    labels = np.unique(labeled_set.y)
    k = labels.shape[0]
    n = labeled_set.size()
    starting_points = [0] * k
    ending_points = [-1] * k

    r = [0] * labels.shape[0] # remaining examples to add in each class

    for q in range(k):
        examples = labeled_set.x[np.where(labeled_set.y == labels[q]),:][0]
        r[q] = examples.shape[0]

    for i in range(10):
        dataset = LabeledSet(labeled_set.getInputDimension())

        for q in range(k):
            examples = labeled_set.x[np.where(labeled_set.y == labels[q]),:][0]
            p = int(round(examples.shape[0] / 10.0))
            if (i == 9) and (r[q] != p):
                ending_points[q] = examples.shape[0]
                r[q] = 0
            else:
                ending_points[q] = starting_points[q] + p
                r[q] -= p

            dataset.addExamples(examples[starting_points[q]:ending_points[q],:], np.array([[labels[q]]] * (ending_points[q] - starting_points[q])))


            starting_points[q] = ending_points[q]
        sets.append(dataset)
    return sets

def get_folds(labeled_set, nb):
    '''
        labeled_set : labeled set to split
        split labeled_set into ten folds
    '''
    sets = []
    labels = np.unique(labeled_set.y)
    k = labels.shape[0]
    n = labeled_set.size()
    starting_points = [0] * k
    ending_points = [-1] * k

    r = [0] * labels.shape[0] # remaining examples to add in each class

    for q in range(k):
        examples = labeled_set.x[np.where(labeled_set.y == labels[q]),:][0]
        r[q] = examples.shape[0]

    for i in range(nb):
        dataset = LabeledSet(labeled_set.getInputDimension())

        for q in range(k):
            examples = labeled_set.x[np.where(labeled_set.y == labels[q]),:][0]
            p = int(round(examples.shape[0] / nb))
            if (i == nb-1) and (r[q] != p):
                ending_points[q] = examples.shape[0]
                r[q] = 0
            else:
                ending_points[q] = starting_points[q] + p
                r[q] -= p

            dataset.addExamples(examples[starting_points[q]:ending_points[q],:], np.array([[labels[q]]] * (ending_points[q] - starting_points[q])))


            starting_points[q] = ending_points[q]
        sets.append(dataset)
    return sets

