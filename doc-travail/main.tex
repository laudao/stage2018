\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{float}

\setlength{\parindent}{1em}
%\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\algnewcommand{\algorithmicand}{\textbf{ and }}
\algnewcommand{\algorithmicor}{\textbf{ or }}
\algnewcommand{\OR}{\algorithmicor}
\algnewcommand{\AND}{\algorithmicand}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\newcommand{\myfrac}[2]{\frac{\displaystyle {#1}}{\displaystyle {#2}}}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{graphicx}

\usepackage{enumitem}
\setitemize{label=\textbullet, font=\small}

\usepackage{suffix}
\renewcommand{\eqref}[1]{équation~\ref{#1}}
\newcommand{\algoref}[1]{algorithme~\ref{#1}}
\newcommand{\figref}[1]{figure~\ref{#1}}
\newcommand{\tabref}[1]{tableau~\ref{#1}}
\newcommand{\secref}[1]{section~\ref{#1}}
\newcommand{\probref}[1]{problème~\ref{#1}}
\newcommand{\propref}[1]{proposition~\ref{#1}}
\newcommand{\theoremref}[1]{théorème~\ref{#1}}
\newcommand{\chapref}[1]{chapitre~\ref{#1}}
\WithSuffix\newcommand\algoref*[1]{algorithme~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\figref*[1]{figure~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\eqref*[1]{équation~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\tabref*[1]{tableau~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\secref*[1]{section~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\probref*[1]{problème~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\propref*[1]{proposition~\ref{#1} p.~\pageref{#1}}
\WithSuffix\newcommand\chapref*[1]{chapitre~\ref{#1} p.~\pageref{#1}}

\usepackage[backend=biber,uniquename=init,giveninits=true,
             %% "et al" pour > deux auteurs, & pour exactement 2
             uniquelist=false,maxcitenames=2,mincitenames=1,maxbibnames=99,
             isbn=false,url=false,doi=false,bibstyle=numeric
]{biblatex}
\addbibresource{references.bib}


\begin{document}

%\title{Rapport de stage : \\ Apprentissage et classification monotone}
%\author{Kim-Anh Laura Nguyen \\
%    Licence 3 Informatique \\
%    Sorbonne Université \\
%    Équipe LFI, LIP6
%}

\begin{titlepage}
  %\begin{sffamily}
  \begin{center}

      \makebox[0.5\textwidth][l]{%
        \includegraphics[width=0.2\textwidth]{images/lip6.png}%
      }%
      \makebox[0.5\textwidth][r]{%
        \includegraphics[width=0.33\textwidth]{images/sorbonne.png}%
    }%

    % Title
    \HRule \\[0.4cm]
    { \huge \bfseries Apprentissage et classification monotone\\[0.4cm] }

      \textsc{\LARGE Rapport de stage}\\[0.4cm]

    \HRule \\[0.4cm]
    %\HRule \\[2cm]
    %\includegraphics[scale=0.2]{images/.JPG}
    %\\[2cm]

    % Author and supervisor
    \begin{minipage}{0.4\textwidth}
      \begin{flushleft} \large
        Laura \textsc{Nguyen}\\
        Licence 3 d'Informatique\\
        Promo 2017-2018 \\
      \end{flushleft}
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
      \begin{flushright} \large
        \emph{Encadrant :} Christophe \textsc{Marsala}\\
        \emph{Équipe : } \textsc{LFI} \\
        \emph{Laboratoire : } \textsc{LIP6} \\

      \end{flushright}
    \end{minipage}

      \vspace{2cm}

    % Bottom of the page
    {\large 4 Juin 2018 — 31 Juillet 2018}

  \end{center}
  %\end{sffamily}
\end{titlepage}
%\maketitle

\newpage

\section{Introduction} Dans beaucoup de problèmes de classification, les valeurs
des attributs et de la classe sont ordinaux. De plus, il peut exister une
contrainte de monotonie: la classe d'un objet doit croître/décroître en fonction
de la valeur de tout ou partie de ses attributs.  A savoir, étant donné deux
objets $x, x'$, si $x \leq x'$ alors $f(x) \leq f(x')$. Les variables
dépendantes, $f(x)$ et $f(x')$, sont des fonctions monotones des variables
indépendantes, $x$ et $x'$.
On parle alors de problèmes de classification monotone, ou problèmes de
classification avec contrainte de monotonie. Cette contrainte indique que les
objets ayant de meilleures valeurs d'attributs ne doivent pas être assignés à de
moins bonnes valeurs de classe.\\
L'ajout de cette contrainte de monotonie permet d'introduire des concepts
sémantiques tels la préférence, la priorité, l'importance, qui nécessitent une
relation d'ordre.\\ Il existe de nombreux domaines se prêtant à ce type de
tâches, tels la prédiction du risque de faillite \cite{greco-new-bankruptcy},
l'analyse de la satisfaction des clients \cite{greco-customer}, le diagnostic
médical \cite{marsala-gradual}. 
L'importance de la prise en compte d'une relation graduelle entre les valeurs
d'attributs et la classe a été démontrée \cite{pazzani-acceptance}: les
classifieurs auxquels sont imposés la contrainte de monotonie sont au moins
aussi performants que leurs homologues classiques, et les experts sont plus
enclins à utiliser les règles générées par les modèles monotones.\\
Afin d'extraire des règles à partir de données monotones, on décide d'utiliser
les arbres de décision, dont l'efficacité et l'interprétabilité en
classification a été prouvée \cite{quinlan-induction}.  Cependant, les
algorithmes de construction d'arbres de décision standards (générés par CART
\cite{leo-classification}) ne produisent pas de classifieurs sensibles à la
monotonie, même si la base utilisée est complètement monotone.  En revanche, il
est montré dans \cite{ben-adding} que les classifieurs purement monotones
(\cite{ben-learning}, \cite{ben-monotonicity}, \cite{cao-consistent}) sont, en
terme de taux de bonne classification, statistiquement indiscernables de leurs
homologues non-monotones.  Dans le même article, il est expliqué que ce
phénomène est dû à la sensibilité de ces classifieurs au bruit non-monotone
présent dans les données réelles. \\

Ce stage a pour but d'étudier la construction et l'évaluation d'arbres de
décision prenant en compte une relation graduelle susceptible d'exister entre
les valeurs d'attributs et la classe, tout en étant suffisamment robuste au
bruit non-monotone. On reprend, en particulier, \cite{marsala-rank} pour la
construction d'arbres de décision monotones paramétrés par une mesure de
discrimination d'ordre. Une étude théorique des propriétés des mesures présentées
dans le même article est également effectuée.\\

\section{Etat de l'art} 
Dans cette partie, on étudie et compare les méthodes proposées par Hu et al.
\cite{hu-rank}, Marsala et Petturiti \cite{marsala-rank}, Qian et al.
\cite{qian-fusing} et Pei et Hu \cite{pei-partially}.

\subsection{Notations}
On considère un ensemble $\Omega = \{\omega_1,...,\omega_n\}$ de~$n$ d'éléments définis
par un ensemble de~$m$ attributs $A = \{a_1,...,a_m\}$, où pour tout $j=1,...,m, a_j$
est une fonction de $\Omega$ vers $X_j = \{x_{j_1},...,x_{t_j}\}$. On note aussi
$\lambda: \Omega \rightarrow C$ la fonction d'étiquetage, où $C =
\{c_1,...,c_k\}$ est un ensemble de classes totalement ordonné.

Pour $\omega_i \in \Omega$, l'ensemble dominant de $\omega_i$ généré par $a_j$
est défini (\cite{greco-roughappr}, \cite{greco-roughsets}) de la façon suivante :

$$[\omega_i]^{\leq}_{a_j} = \{w_h \in \Omega : a_j(\omega_i) \leq a_j(\omega_h)\}$$

De même, l'ensemble dominant de $\omega_i$ généré par $\lambda$ s'écrit :

$$[\omega_i]^{\leq}_{\lambda} = \{w_h \in \Omega : \lambda(\omega_i) \leq
\lambda(\omega_h)\}$$ 

Les notions de \textit{dominance rough sets} sont également utilisées. On
cherche à approximer l'ensemble $c^{\geq}_q$, i.e. l'ensemble des éléments de
$\Omega$ dont la valeur de classe est inférieure à $c_q$.

\noindent Soit $B \subseteq A$ et $c_q$ une valeur de classe. Les approximations
inférieure et supérieure de $c^{\geq}_q$ sont définis de la façon suivante:

$$ \underline{R^{\geq}_B}(c^{\geq}_q) = \{\omega \in \Omega :
[\omega]^{\geq}_B \subseteq c^{\geq}_q\}$$

$$ \overline{R^{\geq}_B}(c^{\geq}_q) = \{\omega \in \Omega :
[\omega]^{\leq}_B \cap c^{\geq}_q \neq \varnothing\}$$


\subsection{Arbre de décision basé sur l'entropie d'ordre}
Il est montré dans \cite{ben-adding} que les classifieurs purement monotones ne
procurent pas de meilleurs résultats que leurs homologues classiques lorsque les
données contiennent un taux élevé de bruit non-monotone. 

Afin de résoudre ce problème, les auteurs de \cite{hu-rank} proposent un algorithme robuste pour la
classification monotone.

Pour cela, ils définissent une mesure de degré de monotonie entre deux
attributs, qu'ils nomment information mutuelle d'ordre, ou
\textit{rank mutual information} (RMI): \\
$$ RMI^{\leq}(a_j, a_{j'}) = -\frac{1}{n} \sum_{i=1}^{n} \log
\frac{|[\omega_i]^{\leq}_{a_j}]| \times |[\omega_i]^{\leq}_{a_{j'}}]|}{n \times
|[\omega_i]^{\leq}_{a_j}]\cap [\omega_i]^{\leq}_{a_{j'}}]|}$$

RMI, sensible à la monotonie et robuste face aux données bruitées, est utilisé
comme mesure de sélection d'attribut dans leur algorithme de construction
d'arbres de décision monotones, REMT (\textit{Rank Entropy Based Decision
Tree}).

Dans cet article, on considère seulement les arbres binaires dans lesquels
est affecté à chaque noeud un seul attribut. Concernant les données, les
attributs doivent être numériques et la classe, ordinale.


\noindent À chaque étape de l'induction :
\begin{itemize}
    \item si $|\Omega_{\alpha}| = 1$ ou tous les éléments
de $\Omega_{\alpha}$ partagent la même classe, une feuille est créée. 
    \item Sinon, pour chaque attribut $a_j$, pour chaque valeur d'attribut $x_{j_s}$, l'ensemble
des éléments est divisé en deux sous-ensembles à partir desquels on calcule
$RMI_{x_{j_s}} = RMI(a^{x_{j_s}}_j, \lambda)$. On récupère, pour chaque $a_j$,
le seuil de coupure $x^*_j$ maximisant l'information mutuelle d'ordre :
$x^*_{j_s} = arg\,max RMI_{x_{j_s}}.$ L'attribut $a^*$ utilisé pour le
partitionnement est celui dont le seuil $x^*$ maximise
$RMI(a^{x_{j_s}}_j,\lambda)$, pour $j=1,...,m, s=1,...t_j-1.$ 
    \item Si $RMI(a^*,
\lambda) < \epsilon$, une feuille est créée. 
    \item Sinon, un noeud est construit et la
procédure est répétée sur les sous-ensembles induits par $a^*$ et $x^*$. \\
\end{itemize}

Concernant le critère d'étiquetage, si tous les exemples de la feuille possèdent
la même classe, on lui assigne cette dernière. Sinon, si les exemples
proviennent de classes différentes, on assigne la classe médiane. Dans le cas où
deux classes possèdent le même nombre d'exemples et la feuille courante provient
de la branche gauche de son noeud parent, on lui assigne la pire classe. Sinon,
on lui affecte la meilleure.  \\

L'approche de construction gloutonne du classifieur ne permet pas d'obtenir un
arbre globalement monotone, même si les données utilisées sont monotones. En
revanche, cette méthode garantit une monotonie plus faible : il est montré que
si la base de données utilisée lors de la construction de l'arbre est
monotone-consistente, alors les règles générées par REMT sont monotones. \\

Afin d'évaluer la performance de leur algorithme, Hu et al. utilisent l'erreur absolue moyenne, ou
\textit{Mean Absolute Error} (MAE) :
$$ MAE = \frac{1}{n} \sum_{i=1}^{n}|f(\omega_i) - \lambda(\omega_i)| $$
où $f(\omega_i)$ est la classe prédite par REMT. \\

L'algorithme est testé sur des bases de données artificielles et réelles. 

\noindent Les bases de données artificielles générées sont monotones. Elles contiennent
1000 exemples à deux attributs, et un nombre de classes variant de 2 à 30. 

\noindent REMT est ici comparé à CART, Rank Tree \cite{xia-ranking}, OLM
\cite{ben-learning} et OSDL \cite{cao-supervised}. Les expériences montrent que
REMT produit le moins d'erreurs parmi ces algorithmes, hormis dans le cas à deux
classes.  Les auteurs soulignent aussi que REMT est le plus précis des quatre
algorithmes, peu importe le nombre d'éléments utilisés lors de la construction
de l'arbre.


Sur des bases de données réelles, Hu et al. comparent leur modèle à CART et Rank
Tree \cite{xia-ranking}. Les expériences menées montrent que REMT donne de
meilleurs résultats en termes de MAE sur 10 bases sur les 12 collectées. De
plus, les auteurs montrent également que plus la taille de la base
d'apprentissage diminue, plus l'écart de performance se creuse entre REMT et les
autres modèles. 
\noindent Enfin, dans le but de comparer les performances des algorithmes
lorsque les données sont monotones, les mêmes bases sont modifiées à l'aide d'un
algorithme de monotonization . Les résultats des expérimentations montrent que
REMT est plus performant que les autres modèles. De plus, les valeurs de
MAE produites par tous les modèles diminuent lorsque les données sont
monotisées.

\subsection{Arbre de décision paramétré par une mesure de discrimination
d'ordre} 
Dans \cite{marsala-rank}, Marsala et Petturiti étudient l'influence de
la mesure de discrimination utilisée lors de la construction du classifieur, en
termes de taux de bonne prédiction et de monotonie. 

Comme les mesures étudiées dans l'article partagent toutes la même structure
fonctionnelle, les auteurs définissent un modèle de construction hiérarchique
permettant d'isoler les propriétés d'une mesure de discrimination d'ordre et
d'en créer de nouvelles. \\
Soit $H^*$ une mesure de discrimination d'ordre. Le pouvoir de discrimination de
l'attribut $a_j$ envers $\lambda$ selon $H^*$ peut donc s'écrire de la façon suivante :
$$ H^*(\lambda | a_j) = h^*(g^*(f^*(\omega_1)),...,g^*(f^*(\omega_n)))$$

où $f^*$ mesure la monotonie locale d'un objet par rapport à la classe, $g^*$
est une transformation strictement décroissante de $f^*$, et $h^*$ agrège les
mesures $g^*$. \\

Marsala et Petturiti proposent un algorithme de construction d'arbre de décision
nommé RDMT (\textit{Rank Discrimination Measure Tree}) essentiellement basé sur
REMT mais qui utilise, à la place de RMI, une mesure de discrimination passée en paramètre.

\noindent Comme précédemment, seuls les arbres binaires à un attribut par noeud
sont considérés. Ce classifieur gère à la fois les attributs numériques et
ordinaux, mais la classe doit être ordinale. Les données incomplètes ne sont pas
acceptées.\\

Notons H la mesure de discrimination utilisée, $\epsilon$ le seuil minimal pour
H, $\Omega_{\alpha}$ l'ensemble des exemples considérés à l'étape courante de
construction, $n_{min}$ la taille minimale pour $\Omega_{\alpha}$, $\delta$ la
profondeur courante de la branche, et $\Delta$ la profondeur maximale que peut
avoir une branche de l'arbre. \\

\noindent A chaque étape de l'induction :
\begin{itemize}
    \item si $|\Omega_{\alpha}| < n_{min}$ ou tous les éléments
de $\Omega_{\alpha}$ partagent la même classe ou $\delta > \Delta$, une feuille est créée. 
    \item Sinon, pour chaque attribut $a_j$, pour chaque valeur d'attribut $x_{j_s}$, l'ensemble
des éléments est divisé en deux sous-ensembles à partir desquels on calcule
        $H(\lambda|a^{x_{j_s}}_j)$. On récupère, pour chaque $a_j$,
le seuil de coupure $x^*_j$ minimisant la valeur de H:
$x^*_{j} = arg\,min H(\lambda|a^{x_{j_s}}_j)$. L'attribut $a^*$ utilisé pour le
partitionnement est celui dont le seuil $x^*$ minimise
$H(\lambda|a^{x_{j_s}}_j)$, pour $j=1,...,m, s=1,...t_j-1.$ 
    \item Si $H(\lambda|a^*) < \epsilon$, une feuille est créée. 
    \item Sinon, un noeud est construit et la
procédure est répétée sur les sous-ensembles induits par $a^*$ et $x^*$. \\
\end{itemize}

\noindent Comme REMT, RDMT ne garantit pas un classifieur globalement monotone, mais
un arbre générant des règles monotones.  \\

Les auteurs évaluent les modèles construits selon trois critères: le taux de bonne
classification (mesuré par le pourcentage d'exemples correctement étiquetés, le
test du K, et MAE), la monotonie et la taille des arbres (en termes de nombre de
feuilles). 
\noindent La monotonie d'un arbre est évaluée par deux mesures :
\begin{itemize}
    \item L'index I de non-monotonie d'un arbre $\mathcal{T}$:  
        $$ I(\mathcal{T}) = \frac{\sum_{u=1}^q \sum_{v=1}^{q} m_{u,v}}{q^2 - q}$$
        où q est le nombre de feuilles de $\Gamma$ et $m_{u,v}$ vaut 1 si les
        feuilles $l_u, l_v$ ne sont pas monotones, 0 sinon.
    \item L'index NMI1 des exemples dans chaque base de test $\mathcal{D}$:
        $$ NMI1(\mathcal{D}) = \frac{\sum_{i=1}^n \sum_{h=1}^{n} NMP(\omega_i,
        \omega_h)}{n^2 - n}$$
        où $NMP(\omega_i, \omega_h)$ vaut 1 si la paire $\omega_i, \omega_h$
        n'est pas monotone, 0 sinon. \\
\end{itemize}

Les expérimentations menées dans l'article visent à comparer les arbres obtenus
par différentes mesures ($H^*_S, H_S, H^*_G, H_G, H^*_P, H^{10}_{MID}, H_{ICT}$).

11 bases artificielles sont générées, dont le taux de NMI augmente de 0 à 10\%
avec un pas de 1\%. Chaque base contient 500 exemples à 5 attributs et 5
classes. \\
\noindent Les tests menés sur ces bases montrent qu'il n'existe aucune
différence significative entre les mesures en ce qui concerne les indices de
taux de bonne classification. De plus, les mesures de discrimination d'ordre
produisent des arbres plus monotones, en particulier lorsque le taux de bruit
non-monotone de la base augmente. Enfin, il est également montré que ces mesures
de discrimination produisent des arbres contenant plus de feuilles que les
mesures classiques.

15 bases de données réelles sont séléctionnées pour évaluer les mesures sur des
cas d'applications concrets. Comme précédemment, il n'y a pas de différence
de taux de bonne classification significative entre les mesures. Cependant,
$H^*_S, H^*_G,$ et $H^*_P$ génèrent des arbres plus monotones et comportant
davantage de feuilles. \\

Les arbres produits par des mesures de discrimination d'ordre ne sont pas
destinés à être directement utilisés pour la classification monotone. En
revanche, étant donné leur degré de monotonie plus élevé, il est avantageux de
les passer en entrée d'algorithmes de post-traitement afin renforcer la
monotonie globale.

\subsection{Fusion d'arbres de décision monotones} 
Afin d'améliorer la capacité de généralisation du modèle de classification, Qian
et al. dans \cite{qian-fusing} proposent une méthode d'ensemble par fusion
d'arbres de décision monotones, notée FREMT (\textit{Fusing rank entropy based
monotonic decision trees}).

Les auteurs couplent une méthode de réduction d'attributs préservant l'ordre
avec REMT, utilisé pour générer les classifieurs de base.

Pour réduire l'ensemble des attributs, ils se basent sur les \textit{dominant
rough sets} et mettent à jour leur définition. Ils introduisent un paramètre
$\beta$ qui permet de paramétrer la sensibilité du \textit{rough set} au bruit
non-monotone. L'ensemble à approximer est : 
$$c^{\geq}_q = \bigcup_{u \leq q} C_u$$
où $C_u \in \Omega / \lambda = \{C_1, C_2,...,C_r\}$ tel que, pour tout $q, u \leq r$, si $q
\geq u$ alors les éléments de $C_q$ sont préférés à ceux de $C_u$.

\noindent Soit $B \subseteq A$ et $c_q$ une valeur de classe. Les approximations
inférieure et supérieure de $c^{\geq}_q$ sont définis de la façon suivante:

$$ \underline{R^{\geq}_B}(c^{\geq}_q) = \{\omega \in \Omega :
\frac{|[\omega]^{\geq}_B \cap c^{\geq}_q|}{|[\omega]^{\geq}_B|} \geq 1 -
\beta\}$$

$$ \overline{R^{\geq}_B}(c^{\geq}_q) = \{\omega \in \Omega :
\frac{|[\omega]^{\geq}_B \cap c^{\geq}_q|}{|[\omega]^{\geq}_B|} \geq \beta\}$$

\noindent où $0 \leq \beta \leq 0.5$

\noindent La frontière ascendante de $c_q$ liée à l'ensemble d'attributs B s'écrit :

$$ BND^{\beta}_B(c^{\geq}_q) = \overline{R^{\geq}_B}(c^{\geq}_q) - \underline{R^{\geq}_B}(c^{\geq}_q)$$

\noindent La dépendance monotone de $\lambda$ selon B est donc exprimée de la façon
suivante :

$$ \gamma^{\beta}_B(\lambda) = \frac{|\Omega - \bigcup_{q=1}^k
BND^{\beta}_B(c^{\geq}_q)|}{|\Omega|}$$

\noindent A partir de cette définition, il est possible d'établir un coefficient de
pertinence d'un attribut a dans B, relativement à $\lambda$. La pertinence
interne de a dans B relativement à $\lambda$ est définie de la manière suivante
:

$$ Sig^{\beta}_{inner}(a, B, \lambda) = \gamma^{\beta}_B(\lambda) -
\gamma^{\beta}_{B-\{a\}}(\lambda)$$

\noindent De la même façon, $\forall a \in A-B$, la pertinence externe de a selon B
correspond à :

$$ Sig^{\beta}_{outer}(a, B, \lambda) = \gamma^{\beta}_{B \cup \{a\}}(\lambda) -
\gamma^{\beta}_B(\lambda)$$ \\

À l'aide de ces mesures de pertinence d'attributs, Qian et al. proposent un
algorithme de réduction d'attributs, paramétré par $\beta$, qui, étant donné
$\Omega$,~$A$ et $\lambda$, génère une réduction (\textit{reduct}) d'attributs
monotones. Le principe est le suivant :

\begin{itemize}
    \item On récupère dans~$B$ les attributs noyaux de~$B$ en utilisant
        $Sig^{\beta}_{inner}$.
    \item Pour tout attribut $a_j \in A-B$, on calcule $Sig^{\beta}_{outer}(a_j,
        B, \lambda)$. 
    \begin{itemize}
        \item Si la valeur calculée est supérieure ou égale à toutes celles obtenues
            précédemment, alors on rajoute $a_j$ dans~$B$.
        \item on réitère le processus jusqu'à ce que, pour tout $a_i \in A-B$,
            $Sig^{\beta}_{outer}(a_i, B, \lambda) = 0$ (i.e. rajouter $a_i$
            à~$B$ ne modifie pas $\gamma^{\beta}_{B}(\lambda)$).
    \end{itemize}
\end{itemize}

\noindent Comme la taille de l'ensemble des attributs monotones sélectionnés par cette
méthode est bien moindre que celle de l'ensemble original, l'arbre de décision
induit par ces attributs sera moins profond et contiendra moins de noeuds, ce
qui implique une meilleure aptitude à généraliser. Chaque arbre produit par cet
algorithme sert de classifieur de base pour la méthode d'ensembles.

Afin d'obtenir un classifieur final performant, il est nécessaire de produire
des arbres les plus diversifiés possibles. Plusieurs sous-ensembles d'attributs
sont donc sélectionnés, et chacun de ces sous-ensembles est une réduction d'attributs
qui préserve la monotonie des données. 

\noindent Les auteurs définissent donc une méthode de recherche de multiples réductions
d'attributs monotones, paramétrée par $\beta$. Étant donné $\Omega$,~$A$ et
$\lambda$, elle consiste à :
\begin{itemize}
    \item Trouver les attributs noyaux de A en utilisant $Sig^{\beta}_{inner}$.
    \item Trier par ordre croissant les attributs restants $a_j \in B$ en fonction de
        $\gamma^{\beta}_{B} + \frac{|\Omega / \{a_j\}|}{|\Omega|}$.
    \item Stocker dans $P^*$ les deux sous-ensembles précédents.
    \item À partir de $P^*$, trouver une réduction
d'attributs $RED_0$ avec l'algorithme de réduction d'attributs donné plus haut.
    \item Pour chacun des attributs $a_j$ se trouvant dans $RED_0$ mais n'étant
        pas des attributs noyaux,
    \begin{itemize}
        \item Enlever $a_j$ de $P^*$.
        \item Calculer une réduction $RED_j$ à partir de $P^*$ grâce à
            l'algorithme précédent.
        \item Si $RED_j$ n'a pas déjà été trouvée précédemment, rajouter $RED_j$
            à l'ensemble des réductions obtenues.
        \item Remettre $a_j$ dans $P^*$.
    \end{itemize}
\end{itemize}

\noindent Cette méthode permet de générer un ensemble de réductions d'attributs monotones
aussi diversifié que possible. Ces sous-ensembles d'attributs servent de base pour construire des arbres de
décision complémentaires, générés par REMT. 

Soit $F = \{T_1,T_2,...,T_N\}$ une forêt d'arbres de décision générés par $RED =
\{RED_1,RED_2,...,RED_N\}$. Étant donnés $\Omega$, $A$, $\lambda$,
$\beta = \{\beta_1,\beta_2,\}$, et $\omega$ un objet dont la classe est à
déterminer, l'algorithme final de fusion d'arbres de décision
monotones, \textit{Fusing rank entropy based monotonic decision trees (FREMT)},
est le suivant :
\begin{itemize}
    \item Pour $i=1,...,M$, trouver $RED_i = \{RED_0,RED_1,...,RED_{N_i}\}$ avec
        l'algorithme précédent
    \item Stocker dans $RED$ l'union de tous les ensembles de réductions $RED_i$ générés
    \item Pour chaque $RED_j \in RED$, générer un arbre $T_j$ avec REMT.
    \item Construire F la forêt d'arbres $\{T_1,T_2,...,T_N\}$.
    \item Déterminer la classe de $\omega$ avec F selon le principe de fusion
        basé sur la probabilité maximale (\textit{Fusing principle based on
        maximal probability}). \\
\end{itemize}


Qian et al. évaluent la performance de leur modèle selon le taux de bonne
classification (mesuré par le pourcentage d'instances correctement classifiées)
et l'erreur absolue moyenne. 

Les tests sont menés uniquement sur des bases réelles.

\noindent Sur les 10 bases récupérées, 9 nécessitent un pré-traitement afin
d'être adaptés à l'algorithme de fusion d'arbres (en transformant les problèmes
de monotonie descendantes en problèmes de monotonie ascendantes). De plus, afin
de comparer les performances des modèles lorsque les données sont monotones, les
classes des données sont modifiées à l'aide d'un algorithme de monotonization.

\noindent Les auteurs observent d'abord les performances des arbres constituant
la forêt (construits à partir des réductions d'attributs des données d'origine)
puis la performance du modèle final. Ils comparent leurs résultats à ceux
obtenus par REMT.

\noindent D'après les expériences menées, on observe que les arbres de décision
générés par les réductions d'attributs offrent de meilleurs résultats en termes
de taux de bonne classification et d'erreur absolue moyenne dans la plupart des
cas. De plus, les valeurs générées sont différentes les unes des autres, ce qui
satisfait la contrainte de diversité nécessaire au modèle d'ensembles.
Enfin, le classifieur produit par FREMT s'avère être meilleur que
celui généré par REMT sur toutes les bases considérées, en termes de taux de
bonne classification et d'erreur absolue moyenne. FREMT permet aussi d'améliorer
la capacité de généralisation de REMT sur ces tâches de classification.


\subsection{Arbres de décision partitiellement monotones} 

Les algorithmes étudiés précédemment font l'hypothèse que tous les attributs
sont monotones par rapport à la classe. Néanmoins, la plupart des tâches de
classification avec contrainte de monotonie comportent deux sortes d'attributs :
ceux dont les valeurs sont linéairement ordonnées selon les valeurs de la classe
(les critères), et ceux n'ayant pas de relation monotone avec la classe (mais
qui permettent tout de même d'améliorer la performance du classifieur). Dans
\cite{pei-partially}, Pei et Hu proposent donc un algorithme de construction
d'arbres partiellement monotones permettant de gérer les deux types d'attributs
à la fois. 


Pour cela, ils proposent une mesure d'inconsistence d'ordre, \textit{rank
inconsistency rate} (RIR), qui permet de distinguer les attributs ordinaires des
critères et de déterminer les sens des relations graduelles entre les critères
et la classe. Étant donnés $a_j \in A$ et $\lambda$, on définit :

\begin{itemize}
    \item URIR (\textit{upward rank inconstency rate}), le RIR ascendant selon~$a_j$ :
        $$ URIR^{\leq}(a_j, \lambda) = -\frac{1}{|\Omega|} \sum_{i=1}^{n}\log_{2}
        \frac{|[\omega_i]^{\leq}_{a_j}| \times
        |[\omega_i]^{\geq}_{\lambda}|}{|\Omega| \times |[\omega_i]^{\leq}_{a_j}
        \cap [\omega_i]^{\geq}_{\lambda}|}$$ 
    \item DRIR (\textit{downward rank inconstency rate}), le RIR
        descendant selon~$a_j$ : 
        $$ DRIR^{\geq}(a_j, \lambda) = -\frac{1}{|\Omega|} \sum_{i=1}^{n}\log_{2}
        \frac{|[\omega_i]^{\geq}_{a_j}| \times
        |[\omega_i]^{\leq}_{\lambda}|}{|\Omega| \times |[\omega_i]^{\geq}_{a_j}
        \cap [\omega_i]^{\leq}_{\lambda}|}$$
\end{itemize}

$a_j$ croît avec $\lambda$ lorsque
$URIR^{\leq}(a_j,\lambda) = 0$, et décroît avec $\lambda$
quand $DRIR^{\geq}(a_j,\lambda) = 0$.

La différence entre $URIR^{\leq}(a_j,\lambda)$ et $DRIR^{\geq}(a_j,\lambda)$
est représentée par :

$$ diff_{a_j} = URIR^{\leq}(a_j,\lambda) - DRIR^{\geq}(a_j,\lambda)$$

Pei et Hu établissent un seuil $\delta \in [0,1]$ permettant de décider s'il
existe une relation graduelle entre $a_j$ et $\lambda$. Si $diff_{a_j} \in
[-\delta, \delta]$, alors $a_j$ n'est pas monotone par rapport à $\lambda$.
Sinon, si $diff_{a_j} \in [\delta, \infty[$, alors $a_j$ décroît avec $\lambda$.
Au contraire, si $diff_{a_j} \in ]-\infty, -\delta]$, alors $a_j$ croît avec
$\lambda$.

Ces mesures permettent de définir un algorithme capable de déterminer s'il
existe une relation graduelle entre chaque attribut et la classe ainsi que son
éventuel sens de variation.

Afin de partitionner au mieux les données à chaque étape de construction de
l'arbre, les mesures de sélection d'attributs RMI et MI sont utilisées. 

\noindent L'information mutuelle MI (\textit{Mutual Information}) correspond au degré de
consistence entre un attribut et les valeurs de la classe. L'information
mutuelle entre l'attribut $a_j$ et la classe $\lambda$ est défini par :

$$ MI(a_j, \lambda) = -\frac{1}{|\Omega|} \sum_{i=1}^{n} \log_{2}
\frac{|[\omega_i]_{a_j}|
\times |[\omega_i]_{\lambda}|}{|\Omega| \times |[\omega_i]_{a_j} \cap
[\omega_i]_{\lambda}|}$$

\noindent De plus, Pei et Hu utilisent le coefficient d'information maximale MIC
(\textit{Maximal Information Coefficient}) pour enlever les attributs
impertinents. Soient $X$ et $Y$ deux variables aléatoires, $|X|$ et $|Y|$
représentent le nombre d'intervalles sur les axes $X$ et $Y$. Le nombre total
d'intervalles n'excède pas une valeur $T$ spécifiée. MIC est défini par :

$$ MIC(X, Y) = \max_{|X||Y| < T} \frac{MI(X,Y)}{\log_{2}(\min(|X|,|Y|))}$$

À l'aide de ces mesures, les auteurs développent PMDT (\textit{Partially
Monotonic Decision Trees}), un algorithme de construction d'arbres partiellement
monotones. Comme les autres méthodes, celle-ci ne considère que les arbres
binaires où à chaque noeud interne correspond un attribut. Elle ne gère que les
attributs numériques, et les valeurs de classe doivent être ordonnées. \\

Soit $\Omega_{\alpha}$ l'ensemble des exemples considérés à l'étape actuelle de
construction, $\epsilon$ le seuil minimal pour MI et RMI, et $n_{min}$ la taille
minimale pour $\Omega_{\alpha}$.
À chaque étape de l'induction :
\begin{itemize}
    \item Les éléments $\omega_i \in \Omega$ sont normalisés
    \item Les attributs impertinents sont retirés avec $MIC$ afin d'obtenir $\Omega'$
    \item Les attributs monotones et les attributs non-monotones sont
        différenciés en utilisant $RIR$
    \item Si tous les éléments de $\Omega'$ appartiennent à la même classe~$c$,
        alors une feuille est créée et on lui assigne la classe ~$c$
    \item Sinon, si $|\Omega_{\alpha}| < n_{min}$ alors une feuille est créée et
        on lui assigne la classe majoritaire dans $\Omega_{\alpha}$
    \item Sinon
        \begin{itemize}
            \item si le noeud courant n'est pas plus pur que son noeud parent,
        alors le seuil générant la meilleure partition selon MI est récupéré
    \item sinon, le seuil générant la meilleure partition selon RMI est récupéré
        \end{itemize}
    \item Si la partition obtenue est vide ou la valeur de MI ou RMI est plus
        petite que $\epsilon$, alors la construction de l'arbre est arrêtée
    \item Sinon, la même procédure est répétée sur les sous-ensembles de la
        partition \\
\end{itemize}

L'évaluation des modèles est effectuée avec le taux de bonne classification et
MAE.

Pei et Hu comparent PMDT à des méthodes basées sur la structure formelle DRSA
(\textit{dominance-based rough set approach}) DIR-DOMLEM, VC-DomLEM
(\cite{blaszczynski-sequential}, \cite{blaszczynski-induction},
\cite{wang-induction}), ainsi que des algorithmes de construction d'arbres de
décisions tels REMT, RGMT, OLM et OSDL. Ils récupèrent 12 bases réelles dans
lesquelles le sens de variation des critères n'est pas connu, et les
pré-traitent : normalisation des attributs, suppression des données incomplètes,
et transformation des attributs décroissants avec la classe en attributs
croissants.

Les expériences menées montrent que PMDT arrive à gérer à la fois les attributs
non monotones et les critères, et garde les attributs les plus pertinents pour
la construction de l'arbre. MI et RMI gèrent mieux les critères et attributs
non-monotones que DomLEM et VC-DomLEM, et permettent d'améliorer
significativement les performances de classification en termes de taux de bonne
classification et de MAE. De plus, en considérant le fait que les données
puissent être totalement monotones ou non, PMDT produit également de meilleurs
résultats que REMT, RGMT, OLM et OSDL sur la plupart des tâches de
classification traitées.


\section{Etude théorique des propriétés des mesures de discrimination d'ordre}
Marsala et Petturiti définissent dans \cite{marsala-rank} un modèle de
construction hiérarchique de mesures de discrimination d'ordre. Ce modèle a pour
but d'isoler les propriétés qu'une fonction doit avoir pour être une telle
mesure. Il sert aussi de base pour en créer de nouvelles. \\

Soit $H^*$ une mesure de discrimination d'ordre. On rappelle que le pouvoir de
discrimination de l'attribut $a_j$ envers la classe $\lambda$ peut se décomposer
de la façon suivante :

$$ H^*(\lambda|a_j) = h^*(g^*(f^*(\omega_1),...,g^*(f^*(\omega_n))))$$

où $f^*$ est une mesure de la monotonie locale de l'objet, $g^*$ une mesure
de non-monotonie de l'objet, et $h^*$ une agrégation des mesures $g^*$.

Pour que $H^*$ soit une mesure de discrimination d'ordre, chaque couche doit
satisfaire certaines conditions.

Soient $\omega_i \in \Omega$ et $f^*, g^*, h^*$ les trois couches de $H^*$. Les propriétés suivantes sont
démontrées en annexe :


\section{Implémentation et expérimentation de l'algorithme de construction
d'arbres monotones} On implémente une librarie de fonctions permettant la
construction ainsi que l'étude expérimentale d'arbres de décision.  On se base
essentiellement sur RDMT(H), donné dans \cite{marsala-rank}, pour construire des
arbres de décision monotones, que l'on évalue sur des données artificielles et
réelles. Le code est implémenté en Python 3.6 et les librairies numpy
\cite{walt-numpy}, matplotlib \cite{hunter-matplotlib} et scikit-learn
\cite{scikit-learn} sont utilisées.

%\subsection{Aspects calculatoires}
%Dans cette partie, on cherche à évaluer les complexités temporelle et spatiale
%du critère de partitionnement en fonction de la mesure de discrimination
%utilisée. Les mesures de discrimination d'ordre proposées sont basées sur les
%ensembles dominants, et, celles ne tenant pas compte de la monotonie des données,
%sur les classes d'équivalence. Le stockage des ces structures joue donc un rôle
%important dans la complexité du critère de partitionnement. \\
%
%Étant donnés $\Omega = \{\omega_i, ... , \omega_n\}, \mathcal{A} = \{a_1, ...,
%a_m\}, \lambda$, et $j \in \{1, ..., m\}$ fixé, on stocke l'ensemble dominant
%généré par l'attribut $a_j$ (respectivement la fonction d'étiquetage $\lambda$)
%dans une matrice notée~$A$ (respectivement~$D$) de taille $n \times n$. Pour tout $i,
%h \in \{1,...,n\}$,
%\begin{itemize} 
%    \item $A_{ih} = 1$ si et seulement si $a_j(\omega_i) \leq
%a_j(\omega_h)$ ;
%    \item $D_{ih}= 1$ si et seulement si~$\lambda(\omega_i) \leq \lambda(\omega_h)$.
%\end{itemize}
%
%A (respectivement D) est donc rempli en itérant sur chaque $\omega_i \in \Omega$
%et en assignant à chaque ligne $A_i$ (respectivement $D_i$) l'ensemble
%$\{\omega_h : a_j(\omega_i) \leq a_j(\omega_h)\}$ (respectivement $\{\omega_h :
%\lambda(\omega_i) \leq \lambda(\omega_h)\}$). Récupérer ce dernier se fait en
%temps $\mathcal{O}(n)$. On construit donc A et D en temps $\mathcal{O}(n^2)$.
%Comme les valeurs sont stockées dans une matrice de taille $n \times n$, la
%complexité spatiale est aussi en $\mathcal{O}{n^2}$.
%
%En ce qui concerne les classes d'équivalence, pour tout $i, h \in \{1,...,n\}$,
%\begin{itemize}
%    \item $A_{ih} = 1$ si et seulement si $a_j(\omega_i) = a_j(\omega_h)$
%    \item $D_{ih} = 1$ si et seulement si $\lambda(\omega_i) = \lambda(\omega_h)$. 
%\end{itemize}
%La procédure de construction est la même que celle donnée
%précédemment, en remplaçant l'inégalité par l'égalité. On obtient les mêmes
%complexités temporelle et spatiale. Comme les algorithmes maniant les ensembles
%dominants et ceux utilisant les classes d'équivalence partagent les mêmes
%complexités, on étudiera uniquement les fonctions faisant intervenir les
%ensembles dominants. 
%
%Pour $a_j$ donné,~$A$ et~$D$ ne sont calculés qu'une seule fois. \\
%
%Pour calculer $dsr(\omega_i)$, on récupère $[\omega_i]^{\leq}_{a_j}$ et
%$[\omega_i]^{\leq}_{\lambda}$ en temps $\mathcal{O}(n)$. Le calcul de
%$[\omega_i]_{a_j} \cap [\omega_i]_{\lambda}$ étant aussi en $\mathcal{O}(n)$,
%calculer $dsr(\omega_i)$ se fait en temps $\mathcal{O}(n)$. L'espace occupé est
%en $\mathcal{O}(n)$.  En revanche, le calcul de $maxdsr(\omega_i), mindsr(\omega_i)$ et
%$avgdsr(\omega_i)$ nécessite d'itérer sur chaque $\omega_h \in
%[\omega_i]_{a_j}$. Les complexités temporelle et spatiale de ces fonctions sont
%donc en $\mathcal{O}(n^2).$
%
%Toutes les fonctions $g^*$ implémentées sont en $\mathcal{O}(1)$.
%

\subsection{Mesures de discrimination d'ordre} 
On implémente le modèle de
construction hiérarchique proposé par Marsala et Petturiti. On rappelle que le
pouvoir de discrimination de l'attribut $a_j$ envers la classe $\lambda$ selon
une mesure de discrimination $H^*$ peut s'écrire de la façon suivante :

$$ H^*(\lambda | a_j) = h^*(g^*(f^*(\omega_1)),...,g^*(f^*(\omega_n)))$$

\noindent Toutes les fonctions $f^*$, $g^*$, $h^*$ présentées dans l'article
sont codées.  Pour chaque fonction $f^*$, on distingue le cas où la mesure
construite détecte la monotonie (i.e. elle considère les ensembles dominants) du
cas contraire (i.e.  elle considère les classes d'équivalences). \\

Par exemple, étant donnés $a_j \in \mathcal{A}$ et $\lambda$, l'entropie
conditionnelle de Shannon se réécrit de la façon suivante :

$$ H_S(\lambda | a_j) = \sum_{i=1}^{n} \frac{1}{n} (-\log_{2} (\frac{|[\omega_i]_{\lambda} \cap
[\omega_i]_{a_j}|}{|[\omega_i]_{a_j}|}))$$

où $f^*(\omega_i) = \frac{|[\omega_i]_{\lambda} \cap
[\omega_i]_{a_j}|}{|[\omega_i]_{a_j}|}, g^*(\omega_i) = -\log_{2}
(f^*(\omega_i))$, et $h^*(\omega_i) = \sum_{i=1}^{n} \frac{1}{n}
g^*(f^*(\omega_i))$ \\

Sa version qui tient compte la monotonie, l'entropie de Shannon d'ordre, est
définie de la façon suivante :

$$ H^*_S(\lambda | a_j) = \sum_{i=1}^{n} \frac{1}{n} (-\log_{2} (dsr(\omega_i))$$

où $f^*(\omega_i) = dsr(\omega_i) = \frac{|[\omega_i]^{\leq}_{\lambda} \cap
[\omega_i]^{\leq}_{a_j}|}{|[\omega_i]^{\leq}_{a_j}|}$, et $g^*$ et $h^*$ sont
les mêmes que précédemment. \\

Les ensembles dominants et classes d'équivalence servant de base pour le calcul
de $f^*(\omega_i)$, pour tout $\omega_i \in \Omega$, le stockage de ces
structures joue un rôle important dans la complexité du critère de
partitionnement.

Étant donnés $\Omega = \{\omega_i, ... , \omega_n\}, \mathcal{A} = \{a_1, ...,
a_m\}, \lambda$, et $j \in \{1, ..., m\}$ fixé, on stocke les ensembles
dominants générés par l'attribut $a_j$ (respectivement la fonction d'étiquetage $\lambda$) dans une matrice notée~$A$ (respectivement~$D$) de taille $n \times n$. Pour tout $i,
h \in \{1,...,n\}$,
\begin{itemize} 
    \item $A_{ih} = 1$ si et seulement si $a_j(\omega_i) \leq
a_j(\omega_h)$ ;
    \item $D_{ih}= 1$ si et seulement si~$\lambda(\omega_i) \leq \lambda(\omega_h)$.
\end{itemize}
A (respectivement D) est donc rempli en itérant sur chaque $\omega_i \in \Omega$
et en assignant à chaque ligne $A_i$ (respectivement $D_i$) l'ensemble
$\{\omega_h : a_j(\omega_i) \leq a_j(\omega_h)\}$ (respectivement $\{\omega_h :
\lambda(\omega_i) \leq \lambda(\omega_h)\}$). Récupérer ce dernier se fait en
temps $\mathcal{O}(n)$. On construit donc A et D en temps $\mathcal{O}(n^2)$.
Comme les valeurs sont stockées dans une matrice de taille $n \times n$, la
complexité spatiale est aussi en $\mathcal{O}{n^2}$.

En ce qui concerne les classes d'équivalence, pour tout $i, h \in \{1,...,n\}$,
\begin{itemize}
    \item $A_{ih} = 1$ si et seulement si $a_j(\omega_i) = a_j(\omega_h)$
    \item $D_{ih} = 1$ si et seulement si $\lambda(\omega_i) = \lambda(\omega_h)$. 
\end{itemize}
La procédure de construction est la même que celle donnée
précédemment, en remplaçant l'inégalité par l'égalité. On obtient les mêmes
complexités temporelle et spatiale. Comme les algorithmes maniant les ensembles
dominants et ceux utilisant les classes d'équivalence partagent les mêmes
complexités, on étudiera uniquement les fonctions faisant intervenir les
ensembles dominants. 

Pour $a_j$ et $\lambda$ fixés, ~$A$ et~$D$ ne sont calculés qu'une seule fois car
$[\omega_i]^{\leq}_{a_j}$ et $[\omega_i]^{\leq}_{\lambda}$ restent constants,
pour tout $\omega_i \in \Omega$. \\

On s'intéresse maintenant au calcul de $f^*(\omega_i)$, qu'on appelera également
*-$dsr(\omega_i)$.
Afin de calculer $dsr(\omega_i)$, on récupère $[\omega_i]^{\leq}_{a_j}$ et
$[\omega_i]^{\leq}_{\lambda}$ à l'aide de~$A$ et~$D$ en temps $\mathcal{O}(n)$. Le calcul de
$[\omega_i]_{a_j} \cap [\omega_i]_{\lambda}$ se faisant aussi en $\mathcal{O}(n)$,
calculer $dsr(\omega_i)$ se fait en temps $\mathcal{O}(n)$. L'espace occupé est
en $\mathcal{O}(n)$.
En revanche, le calcul de $maxdsr(\omega_i), mindsr(\omega_i)$ et
$avgdsr(\omega_i)$ nécessite d'itérer sur chaque $\omega_h \in
[\omega_i]_{a_j}$. Les complexités temporelle et spatiale de ces fonctions sont
donc en $\mathcal{O}(n^2).$ \\

Afin d'observer le comportement des *-dsr sur un attribut monotone, on génère
une base de 100 points à deux classes, deux dimensions dont une seule est
monotone par rapport à la classe.  Ici, la valeur de l'attribut monotone et
celle de la classe croissent en fonction de l'indice de l'élément. On duplique
et bruite la même base de sorte à obtenir des bases bruitées à 0\%, 25\%, 50\%,
75\%.  Les courbes \ref{img:dsr0}, \ref{img:dsr25}, \ref{img:dsr50}, et
\ref{img:dsr75} tracent la valeur de $dsr(\omega_i)$ en fonction de i pour
chaque base. \\

\begin{figure}[H]
	\center 
	\includegraphics[width=0.6\textwidth]{images/dsr_0.png}
    \caption{Valeur de dsr en fonction de l'indice de l'objet pour une base
    non-bruitée}
    \label{img:dsr0}
\end{figure}

\begin{figure}[H]
	\center 
	\includegraphics[width=0.6\textwidth]{images/dsr_25.png}
    \caption{Valeur de dsr en fonction de l'indice de l'objet pour une base
    bruitée à 25\%}
    \label{img:dsr25}
\end{figure}

\begin{figure}[H]
	\center 
	\includegraphics[width=0.6\textwidth]{images/dsr_50.png}
    \caption{Valeur de dsr en fonction de l'indice de l'objet pour une base
    bruitée à 50\%}
    \label{img:dsr50}
\end{figure}

\begin{figure}[H]
	\center 
	\includegraphics[width=0.6\textwidth]{images/dsr_75.png}
    \caption{Valeur de dsr en fonction de l'indice de l'objet pour une base
    bruitée à 75\%}
    \label{img:dsr75}
\end{figure}

%\begin{figure}[H]
%	\center 
%	\includegraphics[width=0.6\textwidth]{images/dsr_90.png}
%    \caption{Valeur de dsr en fonction de l'indice de l'objet pour une base
%    bruitée à 90\%}
%    \label{img:dsr90}
%\end{figure}

Dans la base non bruitée, dsr($\omega_i$) = 1 pour chaque $\omega_i \in \Omega$.
En effet, on a $a_j(\omega_i) \leq a_j(\omega_h) \Rightarrow \lambda(\omega_i)
\leq \lambda(\omega_h)$ si et seulement si $\{\omega_h \in \Omega :
\lambda(\omega_i) \leq \lambda(\omega_h) \land a_j(\omega_i) \leq
a_j(\omega_h)\} = \{\omega_h \in \Omega : a_j(\omega_i) \leq a_j(\omega_h)\}$ si
et seulement si $\frac{| [\omega_i]^{\leq}_{\lambda} \cap
[\omega_i]^{\leq}_{a_j}|}{| [\omega_i]^{\leq}_{a_j} |} = 1 $.

\noindent En revanche, plus le taux de bruit dans la base augmente, plus on observe de
"pics". En effet, on observe davantage de $\omega_i$ tels que $ \{\omega_h \in
\Omega : a_j(\omega_i) \leq a_j(\omega_h)\} $ est modifié, et donc  $|
\{\omega_h \in \Omega : \lambda(\omega_i) \leq \lambda(\omega_h) \land
a_j(\omega_i) \leq a_j(\omega_h)\} | \leq | \{\omega_h \in \Omega :
a_j(\omega_i) \leq a_j(\omega_h)\} |$.

\noindent Les courbes des autres *-dsr sont données en annexe.\\

Toutes les fonctions $g^*$ présentées dans l'article ont des complexités
temporelle et spatiale en $\mathcal{O}(1)$. En revanche, comme les fonctions
$h^*$ agrègent les couches $g^*$ correspondant à chaque élément de $\Omega$,
elles sont toutes en $\mathcal{O}(n)$.   

Le calcul de $H^*(\lambda | a_j)$ se fait donc en :
\begin{itemize}
    \item $\mathcal{O}(n^2)$ lorsque $f^* = dsr$ 
    \item $\mathcal{O}(n^3)$ lorsque $f^* \in \{mindsr, maxdsr, avgdsr\}$  \\
\end{itemize} 

On étudie, pour chaque couple de mesures de discrimination (H, H') données dans
\cite{marsala-rank}, l'évolution de H' en fonction de H pour 2, 3 et 5 classes.
\\ Pour cela, on génère aléatoirement, pour chaque nombre de classes, 100 bases
de 100 exemples à un attribut monotone par rapport à la classe. Pour chaque
base, on récupère les seuils de coupure engendrés par la discrétisation de
l'attribut (étape décrite dans la section suivante) et, pour
chaque mesure, on enregistre les valeurs obtenues pour chaque seuil.\\ Les
figures \ref{img:H_2}, \ref{img:H_3}, \ref{img:H_5} tracent les corrélations
entre chaque mesure de discrimination. \\

\begin{figure}[H]
	\center 
	\includegraphics[width=0.6\textwidth]{images/H_2.png}
    \caption{H' en fonction de H (2 classes)}
    \label{img:H_2}
\end{figure}

\begin{figure}[H]

	\center 
	\includegraphics[width=0.6\textwidth]{images/H_3.png}
    \caption{H' en fonction de H (3 classes)}
    \label{img:H_3}
\end{figure}

\begin{figure}[H]

	\center 
	\includegraphics[width=0.6\textwidth]{images/H_5.png}
    \caption{H' en fonction de H (5 classes)}
    \label{img:H_5}
\end{figure}

Pour chaque cas, on remarque que les mesures suivantes sont corrélées
linéairement :
\begin{itemize}
    \item $H^*_S$ et $H^*_G$
    \item $H_G$ et $H_S$
\end{itemize}

Et les mesures suivantes sont corrélées quadratiquement :
\begin{itemize}
    \item $H_P$ et $H_M$
    \item $H_Q$ et $H^*_S$
\end{itemize}

\subsection{Algorithme de construction d'arbres de décision paramétré par une
mesure de discrimination}

En reprenant essentiellement RDMT, l'\algoref{alg:build-tree} est un
classifieur paramétré par une mesure de discrimination H (d'ordre ou non) et par
3 critères d'arrêt. Comme RDMT, cet algorithme ne produit pas un arbre
globalement monotone. Néanmoins, si les données sont monotone-consistentes et si
H tient compte de la relation graduelle entre les valeurs d'attributs et les
valeurs de classe, alors cette méthode garantit une forme plus faible de
monotonie (appelée \textit{rule monotonicity}). 

RDMT et notre algorithme diffèrent sur plusieurs points :
\begin{itemize}
    \item On ne traite pas les attributs non numériques
    \item Lorsqu'une feuille est créée, on lui assigne la classe majoritaire parmi les exemples ayant servi à sa
construction
    \item On distingue deux mesures de discrimination: l'une doit
être minimisée pour le partitionnement et l'autre permet de déterminer l'arrêt. Dans la suite, la deuxième mesure utilisée sera toujours $H_S$.
\end{itemize}

Comme l'on se restreint à des arbres de décision binaires, les données doivent
être partitionnées en deux sous-ensembles à chaque étape de l'induction. Cette
partition se fait en divisant l'ensemble courant $\Omega_{\alpha}$ selon
l'attribut $a^*$ qui respecte le plus la contrainte de monotonie, i.e. celui
dont la valeur $x^*_{j_s}$ minimise $H(\lambda|a^{x_{j_s}}_j)$, pour $j=1,...,m,
s=1,...t_j-1$. 

Etant donné une mesure de discrimination $H^*$, \textit{DISCRETIZE}
(~\algoref{alg:discretize}) permet de récupérer, pour chaque $a_j \in
\mathcal{A}$, le seuil de coupure minimisant la valeur de H ainsi que cette
dernière. On parcourt tous les seuils de coupure $x_{j_s}$ de $a_j$
pour trouver $x^*_{j}$ minimisant $H(\lambda|a^{x_{j_s}}_j)$, soit :

$$ x^*_{j} = arg\,min \{H(\lambda|a^{x_{j_s}}_j), s=1,...,t_j -1\}$$

\begin{algorithm}[H]
\caption{Discrétisation}
\label{alg:discretize}
\begin{algorithmic}
\Procedure{discretize}{$H^*, \Omega$}
\State \Comment{$H^*$ : mesure de discrimination construite de façon hiérarchique}
\State \Comment{$\Omega$ : base de données étiquetées}
\State \Comment{$a_j$: attribut à discrétiser}

\State $n\gets$ $| \Omega |$ 
\State $T \gets $ matrice dont la première colonne contient les valeurs de $a_j(\omega_i)$ triées par ordre croissant, la deuxième, les valeurs de $\lambda(\omega_i)$ triées selon $a_j(\omega_i)$, et la troisième, les $i=1,...,n$ triés selon $a_j(\omega_i)$, pour tout $\omega_i \in \Omega$.
\State $U\gets \{a_j(w_i) : \forall \omega_i \in \Omega\}$ \Comment{valeurs uniques de $a_j(\omega_i)$ pour tout $\omega_i \in \Omega$} \\

\State $a^{b}_j(\omega_i) \gets 1, \forall w_i \in \Omega$ \Comment{valeur de l'attribut $a_j$ binarisée i.e $a^{b}_j(\omega_i) = 0$ si $a_j(\omega_i) \leq x_{j_s}$, 1 sinon, pour $x_{j_s}$ fixé}
\State $[\omega_i]^{\leq}_{a^{b}_j} \gets \Omega, \forall w_i \in \Omega$ 
\State $[\omega_i]a_i]^{\leq}_{\lambda} \gets \{\omega_h \in \Omega : \lambda(\omega_i) \leq \lambda(\omega_h)\}$ \\

\State $S \gets \varnothing $ \Comment{seuils de coupure considérés}
\State $E \gets \varnothing $ \Comment{valeurs de $H^*$ obtenues pour chaque seuil de coupure $s \in S$}
\State V $\gets \varnothing $ \Comment{ensemble des $\omega \in \Omega$ déjà visités} \\

\For{$v \in U$} \Comment{on considère chaque valeur unique de $\mathcal{A}$}
    \State $\Omega_{v} \gets \{\omega_i \in \Omega : a_j(\omega_i) = v\}$
    \State $C_{v} \gets \{c \in C : \exists \omega_h \in \Omega_{v}, \lambda(\omega_h) = c\}$
    \State $v' \gets$ valeur suivante dans U
    \State $\Omega_{v'} \gets \{\omega_i \in \Omega : a_j(\omega_i) = v'\}$
    \State $C_{v'} \gets \{c \in C : \exists \omega_h \in \Omega_{v'}, \lambda(\omega_h) = c\}$
    \State $V \gets V \cup \{\Omega_{v}\}$    
    \State $a^{b}_j(\omega_i) \gets 0$ pour tout $\omega_i \in \Omega_{v}$ 
    \State $x_{j_s}\gets \frac{v + v'}{2}$\\
    
    \If{$C_{v'} \neq C_{v}$}
    	\State $[\omega_i]_{a^{b}_j} \gets \Omega, \forall \omega_i \in V$
        \State $\bar{V} \gets \Omega \setminus V$ \Comment{ensemble des $\omega \in \Omega$ non visités}
        \State $[\omega_i]^{\leq}_{a^{b}_j} \gets \bar{V}, \forall \omega_i \in \bar{V}$
        \State $S \gets S \cup \{x_{j_s}\}$
        \State $E \gets E \cup \{H^*(\lambda | a^{b}_j)\}$
    \EndIf
\EndFor

\State \textbf{return} $\min E, arg\,min_{s \in S} E $ \Comment{retourner la valeur d'entropie minimale et le seuil de coupure permettant de l'obtenir}
\EndProcedure
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\caption{Construction de l'arbre}
\label{alg:build-tree}
\begin{algorithmic}
\Procedure{build\_dt}{$\Omega_{\alpha}$, $H^*$, H, $\epsilon$, $\Delta$, $n_{min}$, $\lambda$, $\delta$}
\State \Comment{$\Omega_{\alpha}$ : base de données étiquetées}
\State \Comment{$H^*$ : mesure de discrimination utilisée pour le
    partitionnement}
\State \Comment{$H$ : mesure de discrimination utilisée pour déterminer l'arrêt }
\State \Comment{$\epsilon$ : limite inférieure pour H}
\State \Comment{$\Delta$ : longueur maximale d'un chemin de la racine à une feuille}
\State \Comment{$n_{min}$ : taille minimale pour $\Omega_{\alpha}$}
\State \Comment{$\lambda$ : fonction d'étiquetage}
\State \Comment{$\delta$ : longueur du chemin de la racine au noeud courant}

\State $h \gets H(\Omega_{\alpha}, \lambda)$

\If{$h < \epsilon$ \OR $| \Omega_{\alpha} | < n_{min}$ \OR $\forall \omega_i,
    \omega_h \in \Omega_{\alpha}, \lambda(\omega_i) = \lambda(\omega_h)$ \OR $\delta > \Delta$}
    \State $c_{\alpha} \gets c$ tel que $|\{\omega_i \in \Omega_{\alpha}:
        \lambda(\omega_i) = c\}| \geq |\{\omega_i \in \Omega_{\alpha}:
        \lambda(\omega_i) \neq c\}|$
    \State \textbf{return} LEAF($c_{\alpha}, \Omega$) \\
\EndIf

\State $m\gets$ nombre d'attributs dans $\Omega$
\State $S\gets \varnothing $ \Comment{pour chaque $a_j \in \mathcal{A}$,
    contient $x_{j_{*}}= arg\,min \{H(\lambda | a^{x_{j_s}}_j), s=1,...,t_{j} - 1$\}}
\State $E\gets \varnothing $ \Comment{pour chaque $a_j \in \mathcal{A}$,
    contient $min \{H(\lambda | a^{x_{j_s}}_j), s=1,...,t_j - 1\}$}\\

\For{$a_j$=0 to m-1}
    \If{$a_j(\omega_i) = a_j(\omega_h), \forall \omega_i, \omega_h \in
    \Omega_{\alpha}$} \State $S \gets S \cup \{\infty\}$
    	\State $E \gets E \cup \{\infty\}$
    \Else
        \State $x_{j_*}, h \gets$ DISCRETIZE($H^*, \Omega_{\alpha}, a_j$) 
        \State $S \gets S \cup \{x_{j_*}\}$
        \State $E \gets E \cup \{h\}$ \\
    \EndIf
\EndFor

\State $x_* \gets arg\,min_{s \in S} E$ %seuils[argmin(entropies)]
\State $a^{x_*}_*\gets arg\,min_{a_j \in \mathcal{A}} E $ \\ %argmin(entropies)

\State $\Omega_{\leq}, \Omega_{\geq}\gets$ DIVIDE($\Omega_{\alpha}, a^{x_*}_*,
    x_*$) 

\If{$| \Omega_{\leq} | = 0$}
    \State \textbf{return} $\{c\}$ tel que $|\{\omega_i \in \Omega_{\geq}:
    \lambda(\omega_i) = c\}| \geq |\{\omega_i \in \Omega_{\geq}:
    \lambda(\omega_i) \neq c\}|$
\EndIf

\If{$| \Omega_{\geq} | = 0$}
    \State \textbf{return} $\{c\}$ tel que $|\{\omega_i \in \Omega_{\leq}
    \lambda(\omega_i) = c\}| \geq |\{\omega_i \in \Omega_{\leq}:
    \lambda(\omega_i) \neq c\}|$
\EndIf

\State $\mathcal{T}_{\leq}\gets$ BUILD\_DT($\Omega_{\leq}, H^*, H, \epsilon, \Delta,
    n_{min}, \lambda, \delta +1$) 
\State $\mathcal{T}_{\geq}\gets$
    BUILD\_DT($\mathcal{T}_{\geq}, H^*, H, \epsilon, \Delta, n_{min}, \lambda, \delta
    +1$)
\State $\mathcal{T} \gets$ TREE($\{a^{x_*}_*\}, \mathcal{T}_{\leq}, \mathcal{T}_{\geq}$)
\State \textbf{return} $\mathcal{T}$

\EndProcedure
\end{algorithmic}
\end{algorithm}

Considérons la base de la figure \ref{img:artificial-dataset} contenant 60
points à deux dimensions x et y et à 3 classes. x est l'attribut dont les valeurs sont
monotones par rapport aux valeurs de la classe. Chaque classe est représentée
par le même nombre de points. La classe 1 est
représentée par les points bleus, la 2 par les points oranges, et la 3 par les
points verts. 

\begin{figure}[H]
	\center 
	\includegraphics[width=0.6\textwidth]{images/artificial-dataset.png}
    \caption{Base de données jouet}
    \label{img:artificial-dataset}
\end{figure}

\begin{figure}[H]
	\center 
	\includegraphics[width=0.6\textwidth]{images/threshold_rsdm.png}
    $$H^*_S(\lambda | \mathbf{x}) =  0.19 \leq H^*_S(\lambda | \mathbf{y}) =  0.53$$
    \caption{Seuil de coupure généré par $H^*_S$}
    \label{img:threshold_rsdm}
\end{figure}

\begin{figure}[H]
	\center 
	\includegraphics[width=0.6\textwidth]{images/threshold_sdm.png}
    $$H_S(\lambda | \mathbf{x}) = H_S(\lambda | \mathbf{y}) =  0.67$$
    \caption{Seuil de coupure généré par $H_S$}
    \label{img:threshold_sdm}
\end{figure}

On observe sur les figures \ref{img:threshold_rsdm} et \ref{img:threshold_sdm}
les seuils de coupures générés respectivement par $H^*_S$ et $H_S$ sur cette
base jouet. On remarque que $H^*_S$ tient compte de la monotonie des valeurs de
la classe par rapport aux valeurs de l'attribut x : la valeur d'entropie
minimale calculée pour l'attribut x est inférieure à celle calculée pour
l'attribut y. Ce n'est cependant pas le cas pour $H_S$ qui est insensible à la
monotonie : la coupure optimale sur l'axe x génère la même valeur d'entropie que
celle faite sur l'axe y.

\subsection{Analyse expérimentale} 

Dans cette section, on cherche à comparer entre eux les arbres obtenus en
utilisant $H^*_S$, $H_S$, $H^*_G$, et $H_G$ comme mesure de partitionnement.
L'objectif est d'évaluer l'impact du critère de partitionnement sur les
performances des classifieurs en termes de taux de bonne classification et de
monotonie. On étudie également l'évolution de ces performances en fonction du
taux de bruit non-monotone. 

Dans les expériences menées, les arbres sont construits avec
l'\algoref{alg:build-tree}, en faisant varier la mesure de discrimination parmi
$H^*_S$, $H_S$, $H^*_G$, et $H_G$. Ils sont testés sur des bases artificielles
et réelles.

\subsubsection{Expérimentations sur des bases réelles}

Dans cette partie, nos arbres sont testés sur différentes bases réelles de
classification et une base de régression récupérées sur UCI \cite{uci}. Pour
chaque base, une étape de pré-traitement est effectuée, dans laquelle on
supprime les exemples incomplets ainsi que les attributs non numériques. 

On cherche à déterminer si, pour chaque mesure d'évaluation, les mesures de
discrimination testées produisent des résultats significativement différents.

\begin{table}
\resizebox{\textwidth}{!}{
\begin{tabular}{|*{9}{c|}}
    \hline
       Dataset & & $H^{*}_S$  & $H_S$  & $H^{*}_G$  & $H_G$ & $H^{*}_P$ & $H^{*}_M$  & $H^{*}_Q$ \\
    \hline

     CPU & accuracy  & 68.40 \% $\pm$ 0.04 \% & 66.02 \% $\pm$ 0.02 \% & 66.48 \% $\pm$ 0.05 \% & 66.97 \% $\pm$ 0.04 \% & 66.51 \% $\pm$ 0.00 \% & 60.73 \% $\pm$ 0.07 \% & 67.93 \% $\pm$ 0.02 \% \\

& profondeur  & 10.50 $\pm$ 0.50& 9.00 $\pm$ 0.00& 10.00 $\pm$ 1.00& 8.50 $\pm$ 0.50& 11.00 $\pm$ 0.00& 14.50 $\pm$ 0.50& 11.00 $\pm$ 1.00 \\

    & nombre de feuilles  & 35.00 $\pm$ 2.00& 27.50 $\pm$ 3.50& 36.50 $\pm$ 2.50& 28.50 $\pm$ 4.50& 36.50 $\pm$ 0.50& 41.50 $\pm$ 1.50& 40.50 $\pm$ 3.50 \\

    & ratio paires non-monotones  & 77.99 \% $\pm$ 0.11 \%& 86.00 \% $\pm$ 2.83 \%& 80.86 \% $\pm$ 1.05 \%& 81.80 \% $\pm$ 0.41 \%& 73.23 \% $\pm$ 4.66 \%& 70.73 \% $\pm$ 2.35 \%& 50.64 \% $\pm$ 16.03 \% \\

    & nombre de paires de feuilles  & 9.50 $\pm$ 0.50& 7.50 $\pm$ 1.50& 9.50 $\pm$ 0.50& 7.50 $\pm$ 1.50& 9.50 $\pm$ 0.50& 10.50 $\pm$ 0.50& 12.50 $\pm$ 1.50 \\

    & & & & & & & & \\

     Breast Cancer & accuracy  & 59.17 \% $\pm$ 0.13 \% & 63.77 \% $\pm$ 0.11 \% & 56.31 \% $\pm$ 0.14 \% & 62.03 \% $\pm$ 0.08 \% & 61.40 \% $\pm$ 0.13 \% & 59.80 \% $\pm$ 0.11 \% & 54.80 \% $\pm$ 0.13 \% \\

    & profondeur & 20.00 $\pm$ 1.48& 16.60 $\pm$ 1.74& 20.40 $\pm$ 1.36& 15.10 $\pm$ 1.45& 19.70 $\pm$ 2.24& 20.80 $\pm$ 1.60& 20.10 $\pm$ 1.58 \\

    & nombre de feuilles & 129.20 $\pm$ 4.71& 81.60 $\pm$ 5.99& 129.80 $\pm$ 4.64& 80.10 $\pm$ 4.25& 114.90 $\pm$ 5.34& 127.10 $\pm$ 5.45& 134.90 $\pm$ 5.66 \\

    & ratio paires non-monotones & 84.51 \% $\pm$ 2.87 \%& 80.15 \% $\pm$ 4.76 \%& 84.49 \% $\pm$ 3.07 \%& 78.44 \% $\pm$ 3.88 \%& 86.80 \% $\pm$ 2.86 \%& 83.67 \% $\pm$ 1.80 \%& 81.68 \% $\pm$ 2.81 \% \\

    & nombre de paires de feuilles & 31.90 $\pm$ 2.91& 18.90 $\pm$ 1.81& 31.60 $\pm$ 3.10& 20.60 $\pm$ 1.69& 28.30 $\pm$ 2.00& 32.40 $\pm$ 1.69& 36.30 $\pm$ 2.61 \\

    & & & & & & & & \\

    Cars Evaluation & accuracy  & 81.94 \% $\pm$ 0.04 \% & 81.85 \% $\pm$ 0.06 \% & 82.00 \% $\pm$ 0.04 \% & 78.72 \% $\pm$ 0.09 \% & 87.08 \% $\pm$ 0.04 \% & 88.59 \% $\pm$ 0.07 \% & 78.53 \% $\pm$ 0.03 \% \\

    & profondeur  & 10.50 $\pm$ 0.50& 10.75 $\pm$ 0.43& 10.50 $\pm$ 0.50& 10.25 $\pm$ 0.43& 10.00 $\pm$ 0.00& 11.00 $\pm$ 0.00& 11.00 $\pm$ 0.00 \\
    & nombre de feuilles  & 66.50 $\pm$ 13.16& 37.25 $\pm$ 1.30& 66.50 $\pm$ 13.16& 38.00 $\pm$ 1.58& 52.00 $\pm$ 3.94& 54.00 $\pm$ 3.74& 121.25 $\pm$ 14.04 \\
    & ratio paires non-monotones & 82.17 \% $\pm$ 5.00 \%& 83.82 \% $\pm$ 1.39 \%& 82.76 \% $\pm$ 5.56 \%& 84.68 \% $\pm$ 1.84 \%& 79.25 \% $\pm$ 2.93 \%& 77.33 \% $\pm$ 2.90 \%& 52.19 \% $\pm$ 4.34 \% \\
    & nombre de paires de feuilles & 27.75 $\pm$ 7.36& 11.75 $\pm$ 1.09& 27.75 $\pm$ 7.36& 12.00 $\pm$ 1.41& 18.25 $\pm$ 1.92& 18.00 $\pm$ 2.12& 46.75 $\pm$ 8.79 \\

    \hline

\hline
\end{tabular}}
\caption{Résultats sur les tâches de classification}
\label{tab:resultats-classification}
\end{table}

\begin{figure}[H]
	\center 
	\includegraphics[width=0.6\textwidth]{images/breast-cancer.png}
    \caption{Corrélations entre les attributs de la base \textit{Breast Cancer}
    et la classe}
    \label{img:breast-cancer}
\end{figure}

\begin{figure}[H]
	\center 
	\includegraphics[width=0.6\textwidth]{images/cpu.png}
    \caption{Corrélations entre les attributs de la base \textit{CPU}
    et la classe}
    \label{img:cpu}
\end{figure}

\begin{figure}[H]
	\center 
	\includegraphics[width=0.6\textwidth]{images/cars.png}
    \caption{Corrélations entre les attributs de la base \textit{Cars Evaluation}
    et la classe}
    \label{img:cars}
\end{figure}

Le \tabref{tab:resultats-classification} contient les résultats de tests menés
sur les tâches de classification collectées.

De manière générale, on peut observer que les arbres construits à partir des
mesures de discrimination à rang sont plus profonds, contiennent plus de
feuilles et de paires de feuilles que ceux générés par les mesures d'entropie
classiques. 

Sur la base \textit{Breast Cancer}, les mesures d'entropie classique produisent
de meilleurs résultats en termes de taux de bonne classification et de monotonie
que les mesures de discrimination d'ordre. La prise en compte de la monotonie
des données dans le critère de partitionnement n'offre donc pas d'avantage.
Néanmoins, cette base ne contient que deux valeurs pour la classe, et plusieurs
attributs ne comprennent que peu de valeurs. Il n'y a donc pas de réelle
relation graduelle entre les attributs et la classe. La \figref{img:breast-cancer}
confirme l'absence de corrélation entre les valeurs d'attributs et de classe :
il n'existe pas de contrainte de monotonie dans cette base.

En revanche, $H^*_S$, $H^*_G$, et $H^*_Q$ produisent de meilleurs taux de bonne
classification que les mesures d'entropie classiques sur la base \textit{CPU}.
De plus, sur les données de \textit{Cars Evaluation}, $H^*_S$, $H^*_G$, $H^*_P$
et $H^*_M$ génèrent de meilleures prédictions que les mesures classiques. Sur
ces deux bases, tous les ratio produits par les mesures de discrimination
d'ordre sont plus faibles que ceux produits par les mesures classiques : cela
indique que les arbres générés par une mesure qui prend en compte la relation
graduelle entre les attributs et la classe garantissent une plus forte monotonie
que les autres.

Dans le but de comprendre pourquoi les mesures de discrimination d'ordre
présentent de meilleures performances, on examine les corrélations entre chaque
paire d'attributs, classe incluse. La \figref{img:cpu} montre que, dans la base
\textit{CPU}, les valeurs des attributs sont très étalées et un certain nombre
d'attributs apparaissent corrélés (positivement comme ERP et PRP, ou
négativement comme PRP et MYCT).

Cependant, on peut voir sur la \figref{img:cars} que les attributs de la base
\textit{Cars Evaluation} comprennent peu de valeurs différentes et sont peu
corrélées à la classe : on manque encore d'une explication générale.

\begin{table}
\resizebox{\textwidth}{!}{
\begin{tabular}{|*{6}{c|}}
    \hline
         Nombre de classes & & $H^{*}_S$  & $H_S$  & $H^{*}_G$  & $H_G$ \\
    \hline

     2 classes & accuracy  & 68.93\% $\pm$ 0.11 \% &69.68 \% $\pm$ 0.11 \% & 69.54 \% $\pm$ 0.12 \% & 70.33\% $\pm$ 0.12 \% \\

     & profondeur  & 17.0 $\pm$ 1.22 & 13.5 $\pm$ 0.87 & 17.5 $\pm$ 1.50 & 12.25 $\pm$ 0.43 \\

     & nombre de feuilles & 99.75 $\pm$ 5.07 & 78.25 $\pm$ 5.54 & 102.5 $\pm$ 7.79 & 79.25 $\pm$ 5.67 \\

     & ratio paires non-monotones & 87.82\% $\pm$ 0.01 \% & 87.86\% $\pm$ 0.01
     \%& 88.36\% $\pm$ 0.01 \% & 87.42\% $\pm$ 0.02 \%\\

     & nombre de paires de feuilles  & 28.5 $\pm$ 2.29 & 24.5 $\pm$ 2.87 & 28.5 $\pm$ 2.60 & 29.0 $\pm$ 1.22\\
    
    & & & & & \\

    3 classes & accuracy  & 39.25\%  $\pm$ 0.10 \%& 33.00\% $\pm$ 0.12\% & 35.14 \% $\pm$ 0.07\% & 32.33 \% $\pm$ 0.11\%  \\

     & profondeur  &  20.5 $\pm$ 1.66&  14.75$\pm$1.09& 18.25$\pm$1.48&  15.5 $\pm$2.69 \\

     & nombre de feuilles  & 158.75 $\pm$ 5.12 & 140.5 $\pm$ 1.66 & 164.25 $\pm$
     3.83 & 143.75 $\pm$ 5.72 \\

     & ratio paires non-monotones  & 74.82 \% $\pm$ 0.03\%& 80.74 \% $\pm$ 0.04\%& 75.60 \% $\pm$ 0.04\%& 79.11 \% $\pm$ 0.04\% \\

     & nombre de paires de feuilles  & 44.5 $\pm$ 2.60& 48.25 $\pm$ 1.92& 45.75 $\pm$ 3.03& 47.25 $\pm$ 3.63 \\

    & & & & & \\
    
    7 classes & accuracy  & 31.18 \%$\pm$ 0.08 \%  & 30.37 \%$\pm$ 0.06\%  & 29.37 \%$\pm$ 0.08 \%\% & 29.24\% \%$\pm$ 0.05 \% \\

     & profondeur & 19.0 $\pm$ 1.87 & 15.75 $\pm$ 1.29& 19.5 $\pm$ 1.12& 14.25 $\pm$ 1.30 \\

     & nombre de feuilles  & 171.75 $\pm$ 3.11 & 151.5 $\pm$ 3.77 & 172.5 $\pm$ 3.64 & 157.5 $\pm$ 4.09 \\

     & ratio paires non-monotones  & 53.82 \%$\pm$ 0.05 \%& 56.23 \%$\pm$ 0.02 \%& 51.74\% \%$\pm$ 0.01 \%& 51.89 \% $\pm$ 0.05 \% \\

     & nombre de paires de feuilles & 47.0 $\pm$ 2.23 & 48.25 $\pm$ 1.92 & 47.0 $\pm$ 1.87& 51.25 $\pm$ 3.34\\

    \hline

\end{tabular}}
\caption{Résultats sur la tâche de régression}
\label{tab:resultats-regression}
\end{table}



\section{Conclusion du stage et perspectives} 

Mon stage a porté sur un framework particulier de classification
monotone~\cite{marsala-rank}. Après avoir fait l'état de l'art sur les approches
de classification monotone proposées dans la littérature, je me suis placée dans
le contexte des arbres de décision monotones paramétrés par une mesure de
sélection d'attribut.

J'ai effectué une étude théorique des mesures de discrimination d'ordre afin de
vérifier certaines propriétés clées et d'en extraire d'autres (notamment le sens
de variation). 

J'ai également implémenté
une bibliothèque pour les arbres monotones comprenant le modèle de construction
hiérarchique des mesures~\cite{marsala-rank}, ainsi que l'algorithme de
construction d'arbres monotones~\cite{marsala-rank} légèrement modifié. 

Des expérimentations sur des bases artificielles et réelles ont été menées afin de
mettre en avant les avantages des arbres monotones et comparer les différentes
mesures de discrimination entre elles. Les résultats observés sur les bases
réelles dans lesquelles il existe une forte gradualité entre les attributs et la
classe montrent que les arbres produits par des mesures de discrimination
d'ordre sont plus performants et plus monotones que ceux générés par des mesures
d'entropie classiques. Cependant, les tests lancés sur des bases générées
artificiellement ne permettent pas de souligner l'apport des mesures de
discrimination d'ordre en termes de monotonie des
arbres. \\

Une perspective possible serait de travailler sur ce problème : il peut venir de la façon de partitionner les données (discrétisation en deux sous-ensembles
alors qu'il peut y avoir plus de deux classes) ou de la mesure d'évaluation de la
monotonie utilisée. De plus, notre algorithme considère que tous les attributs
sont monotones par rapport à la classe, ce qui n'est cependant pas le cas dans
la plupart des cas d'applications réelles. \\

Sur un plan plus personnel, ce stage m'a initiée et donné goût à la recherche en
laboratoire. Je tiens à remercier mon maître de stage, Christophe Marsala, ainsi
qu'Arthur Guillon, doctorant à LFI, pour m'avoir aidée et accompagnée durant ce
stage.

\printbibliography \end{document}
