\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\algnewcommand{\algorithmicand}{\textbf{ and }}
\algnewcommand{\algorithmicor}{\textbf{ or }}
\algnewcommand{\OR}{\algorithmicor}
\algnewcommand{\AND}{\algorithmicand}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}


%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{graphicx}

\usepackage[backend=biber,uniquename=init,giveninits=true,
             %% "et al" pour > deux auteurs, & pour exactement 2
             uniquelist=false,maxcitenames=2,mincitenames=1,maxbibnames=99,
             isbn=false,url=false,doi=false,bibstyle=numeric
]{biblatex}
\addbibresource{references.bib}

\title{Document de travail}
\author{Laura Nguyen}

\begin{document}
\maketitle

\section{Introduction}
    Dans beaucoup de problèmes de classification, les valeurs des attributs et de la classe sont ordinaux. De plus, il peut exister une contrainte de monotonie: la classe d'un objet doit croître/décroître en fonction de la valeur de tout ou partie de ses attributs. A savoir, étant donné deux objets $x, x'$, si $x \leq x'$ alors $f(x) \leq f(x')$. On parle alors de problèmes de classification monotone, ou problèmes de classification avec contraintes de monotonie. \\
Il existe de nombreux domaines se prêtant à ce type de tâches, tels la prédiction du risque de faillite \cite{greco-new-bankruptcy}, l'analyse de la satisfaction des clients \cite{greco-customer}, le diagnostic médical \cite{marsala-gradual}. 
L'importance de la prise en compte d'une relation graduelle entre les valeurs d'attributs et la classe a été démontrée \cite{pazzani-acceptance}: les classifieurs auxquels sont imposés la contrainte de monotonie sont au moins aussi performants que leurs homologues classiques, et les experts sont plus enclins à utiliser les règles générés par les modèles monotones.
Afin d'extraire des règles à partir de données monotones, on utilise, pour modèle d'apprentissage, les arbres de décision. Cependant, les algorithmes de construction d'arbres de décision classiques ne produisent pas de classifieurs sensibles à la monotonie, même si le jeu de données utilisé est complètement monotone. De plus, la plupart des vrais ensemble de données ne sont pas monotones, ce qui limite la performance des classifieurs purement monotones, trop sensibles au bruit. \\

\printbibliography
\end{document}
